<!DOCTYPE html>
<html lang="en">
<head>
        <title>A notch above a monkey</title>
        <meta charset="utf-8" />
        <link rel="shortcut icon" href="theme/img/markos.ico" />

        <link rel="stylesheet" type="text/css" href="theme/css/home.min.css" media="screen" />
				<style type="text/css" title="Light theme">
body {background-color: #fbf8e9;color: #30303c;}
body > footer a, body > footer a:visited {color: #ffb665;}
a, a:visited, a:hover {color: #980000;}
body > footer {background-color:#30303c;color:#fbf8e9;}
aside .photo, .error img, .alignleft img, img.alignleft, .alignright img, img.alignright, .aligncenter img, img.aligncenter,
.zemanta-img img, .imageR img, .imageL img {background-color:#fbf8e9;border:1px solid #30303c;}
.post header h1, .posts header h1, .posts > header h1 {border-bottom:1px solid #30303c;}
ol.code {border: 1px solid #30303c;}
body > footer .mini-logo {display: inline-block;width:97px;height:32px;border:0;background:url(theme/img/markos_dark_small_bg.png);overflow:hidden;text-indent:-200px;}
				</style>

				<style type="text/css" title="Dark theme">
body {background-color: #30303c;color: #fbf8e9;}
body > footer a, body > footer a:visited {color: #980000;}
a, a:visited, a:hover {color: #ffb665;}
body > footer {background-color:#fbf8e9;color:#30303c;}
aside .photo, .error img, .alignleft img, img.alignleft, .alignright img, img.alignright, .aligncenter img, img.aligncenter,
.zemanta-img img, .imageR img, .imageL img {background-color:#fbf8e9;}
.post header h1, .posts header h1, .posts > header h1 {border-bottom:1px solid #fbf8e9;}
ol.code {border: 1px solid #fbf8e9;}
body > footer .mini-logo {display: inline-block;width:97px;height:32px;border:0;background:url(theme/img/markos_light_small_bg.png);overflow:hidden;text-indent:-200px;}
				</style>

        <link href="/articles/feeds/atom.xml" type="application/atom+xml" rel="alternate" title="A notch above a monkey Full Atom Feed" />
        <link href="/articles/feeds/rss.xml" type="application/rss+xml" rel="alternate" title="A notch above a monkey Full RSS Feed" />
        <link href="/articles/feeds/general-development-web.atom.xml" type="application/atom+xml" rel="alternate" title="A notch above a monkey Categories Atom Feed" />
</head>

<body>

<h2>Articles in the General development, Web category</h2>

<section class="envelope posts">
    <header>
        <h1><a href="../">A notch above a monkey <strong></strong></a></h1>
    </header><!-- /#banner -->

    <nav class="colophon">
        <h1>By Marko Samastur</h1>
        <ul>
            <li><a href="/">About me</a></li>
            <li><a href="/articles/feeds/atom.xml">Subscribe</a></li>
        </ul>
    </nav>

        <article class="hentry">
            <h1 class="entry-title storytitle"><a href="../www2007.html" rel="bookmark" title="Permalink to WWW2007">WWW2007</a></h1>

            <div class="entry-content post-text">
                <div>
 <p>
  <a href="http://www2007.org/">
   WWW2007
  </a>
  is wrapping up tomorrow, but today is the last day I am attending. It’s been really refreshing to be at a web oriented gathering without hearing anybody say: “Web 2.0″.
 </p>
 <p>
  I enjoyed listening to talks and learned that I have to brush up my math and maybe learn a new thing or two. I still learned a lot despite my personal shortcomings.
 </p>
 <p>
  My original reason for attending was
  <a href="http://airweb.cse.lehigh.edu/2007/">
   AIRWeb07
  </a>
  , a workshop on adversarial information retrieval on web, because I wanted to learn what anti-spam community is working on and what sort of ideas and tools are being discussed.
 </p>
 <p>
  The answer to an outsider like me seems mainly to be a problem of web spam polluting search engines and algorithms trying to discover spam pages from graphs of connected pages.
 </p>
 <p>
  It is a very simplified description, probably even too simplified, and I have to say that I admire the work and ideas done and shown at the conference. At the same time I was very surprised that focus has been so much on links and practically none on characteristics of spam pages themselves (with notable exception of a submission by
  <a href="http://plg.uwaterloo.ca/~gvcormac/">
   Gordon Cormack
  </a>
  ). I understand that crawlers are somewhat out of vogue in research community, but it still seemed kind of odd.
 </p>
 <p>
  It is as if someone trying to know me would completely ignore what I say and solely concentrate on who am I hanging out with and how do I go about it. It seemed to me that the best way would be to combine both, which is what prof. Cormack proposed as well.
 </p>
 <p>
  In a way I think I may understand this focus. It’s very easy to add spam links on an open participatory web and obviously nobody can control what gets published on all those pages out there. So it’s natural for search companies to concentrate on points that they do control, like their index.
 </p>
 <p>
  Still, I feel as if not enough has been done on preventing stuff like comment spam. Better tools would only lead to more isolated spam pages and therefore to an easier extraction of spam networks from an index.
 </p>
 <p>
  Then again, I don’t do this kind of research for a living and may be only talking nonsense.
 </p>
 <p>
  The other thing that’s been on my mind is a general agreement that this is an arms race and one which won’t be won unless financial incentives driving spammers can be eliminated. Question came up if we should try to break our own protections (as cryptographers do) and how would we fare?
 </p>
 <p>
  I have no doubt that people I met fighting spam are much smarter than most of their opponents, which is why I don’t doubt that they could do it. I doubt only if they should do it.
 </p>
 <p>
  It’s not really intuitive that the best thing in an arms race would be to show the other side how to build a nuke. Especially when it seems we are busy enough fighting problem in its current variations. Then again, I’m not convinced it’s a bad idea either.
 </p>
 <p>
  I won’t go much into other talks, as I feel even less qualified to judge and anyone can get a better insight just by reading accepted papers that are all available online. What I heard or read has been in general of a high quality and if this year is anything to go by, then try to attend next one, which will be next April in Beijing.
 </p>
 <p>
  By the way, visiting Banff is a must even without a conference as an excuse, since the place and even more its surroundings are just stunning. And if you do decide to go, do yourself a favor and contact
  <a href="http://www.tarry.ca/">
   Alicyn
  </a>
  for a place to stay.
 </p>
</div>
            </div><!-- /.entry-content -->

            <footer class="post-metadata">
                <ul>
                    <li>Published on <time datetime="2007-05-11T18:17:00+02:00">Fri 11 May 2007</time> </li>
                    <li><a href="../www2007.html">Add your comment</a></li>
                </ul>
            </footer>
        </article>
        <article class="hentry">
            <h1 class="entry-title storytitle"><a href="../transient-nature-of-openid-identities.html" rel="bookmark" title="Permalink to Transient nature of OpenID identities">Transient nature of OpenID identities</a></h1>

            <div class="entry-content post-text">
                <div>
 <p>
  I’m still a fan of OpenID, even though it lacks solid solutions to some of its problems.
 </p>
 <p>
  Much has been said about risk of phishing and possible solutions, but I’ve been more ignorant than some to a transient nature of OpenID identities. Simply put, even if you run your own server, you don’t own your domain. You just rent it. Thus you don’t really own your OpenID URL either, which makes it a rather risky tool where some sort of permanent identity is needed.
 </p>
 <p>
  There are plenty of cases where there’s no such need and OpenID is in my opinion a better solution to them than existing ones. But it isn’t good enough where something less fleeting is required. Maybe
  <a href="http://www.xdi.org/">
   XRI/XDI
  </a>
  is a better answer?
 </p>
 <p>
  I don’t know. I’m still a fan of OpenID. It’s just a bit less useful than I first thought.
 </p>
</div>
            </div><!-- /.entry-content -->

            <footer class="post-metadata">
                <ul>
                    <li>Published on <time datetime="2007-04-15T20:51:00+02:00">Sun 15 April 2007</time> </li>
                    <li><a href="../transient-nature-of-openid-identities.html">Add your comment</a></li>
                </ul>
            </footer>
        </article>
        <article class="hentry">
            <h1 class="entry-title storytitle"><a href="../handling-404.html" rel="bookmark" title="Permalink to Handling 404">Handling 404</a></h1>

            <div class="entry-content post-text">
                <div>
 <p>
  This blog doesn’t use descriptive URLs, which is not the only time I screwed this up for no good reason. In this case it was mainly laziness and smug i-don’t-care-if-people-find-me attitude, but I hadn’t really realized how stupid this decision was until I started thinking about the problem of missing pages (error 404).
 </p>
 <p>
  It always bugged me how useless most 404 pages are. Sure, I could notify the webmaster about a broken link, but that won’t help me find what I’m looking for. Can’t we do better than that?
 </p>
 <p>
  As it happens we often can.  There’s an unlimited number of ways in which visitors can fail to reach their destination, but majority of them probably fall in four categories:
 </p>
 <ul>
  <li>
   content moved elsewhere
  </li>
  <li>
   broken links
  </li>
  <li>
   typos
  </li>
  <li>
   bad memory recall
  </li>
 </ul>
 <p>
  <strong>
   Content moved elsewhere
  </strong>
 </p>
 <p>
  Many believe that pages should be permanent and links shouldn’t break over time. Yet sometimes we either have little control over this or there are good reasons for breakage. We should still try to mitigate such situation by guiding visitors to correct new location when possible.
 </p>
 <p>
  This is done with HTTP redirects. If content was moved only temporary, then response should have a status code of 302 and contain a link in the header to correct current address. If move was permanent, then the same thing is achieved with code 301.
 </p>
 <p>
  Missing pages amount to around 0.6% of hits on this website. They would be around 8% if redirects weren’t used.
 </p>
 <p>
  <strong>
   Broken links
  </strong>
 </p>
 <p>
  Broken links appear when email clients break URL longer than maximum line length or from botched copy operation. This usually means that address is cut off and part we have is incomplete, but otherwise correct. Handling such links can range from trivial to impossible. Let’s look at one Wikipedia link as an example:
 </p>
 <p style="text-indent:20pt;">
  <a href="http://en.wikipedia.org/wiki/Longest_common_subsequence_problem" title="Link to article about longest common sequence problem">
   http://en.wikipedia.org/wiki/Longest_common_subsequence_problem
  </a>
 </p>
 <p>
  Let’s say that link was broken near the end of it and there was
  <em>
   lem
  </em>
  missing in
  <em>
   problem
  </em>
  . Resulting address might not be complete, but there probably aren’t that many Wikipedia articles where such string forms first part of their address and it would be reasonable to assume that Wikipedia could find the right article. Alas it doesn’t.
 </p>
 <p>
  On the other side of the spectrum are impossible or almost impossible guesses. If address was broken anywhere before
  <em>
   Longest
  </em>
  , then we could learn nothing from it about visitors expectations. This would look like a good place to give up.
 </p>
 <p>
  However, if we are lucky, then our visitors came from one of popular search engines, which means their referrer attribute includes search string that led to our page. We can extract those keywords, use them and hopefully find that page or failing that offer a list of related pages. Not perfect, but beats plain “Not here” sign.
 </p>
 <p>
  <strong>
   Typos
  </strong>
 </p>
 <p>
  Typos are what happens when people use keyboards. I can’t live without one, but I still recognize that my fingers and my brain are not always of the same mind about how to spell a word. Letters get added, dropped or just switch places, all being a problem for a program that usually looks for that one perfect match.
 </p>
 <p>
  There’s help. Edit distance algorithms, like
  <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" title="Description of Levenshteins algorithm">
   Levenshtein’s
  </a>
  , can be used to measure how similar two strings really are. Matching then becomes finding the page with shortest distance from a list of its closest neighbors.
 </p>
 <p>
  Downside is that algorithm is fairly computationally expensive and it might take time to find a match. I’ll explore this problem in tomorrows article.
 </p>
 <p>
  <strong>
   Bad memory requests
  </strong>
 </p>
 <p>
  The main purpose of descriptive URLs is the same as catchy domain names. Make it easy to remember address for later use, when bookmarks or browsers autocomplete can’t be used.
 </p>
 <p>
  But memory is notoriously unreliable and it doesn’t work any better with web addresses. So addresses used may vary enough from the right one that they don’t get caught with edit distance algorithms. As an example, someone might try to access aforementioned Wikipedia’s article with:
 </p>
 <p style="text-indent:20pt;">
  http://en.wikipedia.org/wiki/Longest_subsequence_problem
 </p>
 <p>
  Calculated distance between this address and the real one is 7, which is probably more than would be real limit for matching. We can still look at referrer for clues, but we can also use requested address. In our case, we can extract keywords
  <em>
   longest
  </em>
  ,
  <em>
   subsequence
  </em>
  and
  <em>
   problem
  </em>
  and use them to search for possible hits. Wikipedia doesn’t do this either, but neither do I, so I shouldn’t complain.
  <br/>
  <strong>
   Time to wrap it up
  </strong>
 </p>
 <p>
  I believe this approaches make a good case for logical and descriptive addresses, since most of them don’t work (well) otherwise. If someone requests an article with a nonexistent ID 145, it’s impossible to resolve which of existing ones with IDs 155, 245 or 149 he really wanted.
 </p>
 <p>
  Still, sometimes descriptive addresses are not an option. I’d love to hear ideas or practical experience of how to handle such cases.
 </p>
</div>
            </div><!-- /.entry-content -->

            <footer class="post-metadata">
                <ul>
                    <li>Published on <time datetime="2006-11-08T12:31:00+01:00">Wed 08 November 2006</time> </li>
                    <li><a href="../handling-404.html">Add your comment</a></li>
                </ul>
            </footer>
        </article>

<ul class="paginator">
		<li class="newer">
        <a href="../category/general-development-web4.html">&laquo; Newer</a>
		</li>
		<li class="current">
    5 / 7
		</li>
		<li class="older">
        <a href="../category/general-development-web6.html">Older &raquo;</a>
		</li>
</ul>
</section><!-- /#content -->

    <footer>
        <ul>
            <li>&copy; 2005-2016 Marko Samastur</li>
            <li><a href="/articles/feeds/rss.xml" rel="alternate" title="A notch above a monkey Full RSS Feed">Entries feed</a></li>
        </ul>
        <a href="/" class="mini-logo">Back to homepage</a>
    </footer>
</body>
</html>