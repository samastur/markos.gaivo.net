<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A notch above a monkey - General development, Javascript, Python</title><link href="http://markos.gaivo.net/articles/" rel="alternate"></link><link href="/articles/feeds/general-development-javascript-python.atom.xml" rel="self"></link><id>http://markos.gaivo.net/articles/</id><updated>2005-09-16T00:03:00+02:00</updated><entry><title>Latency, bandwidth and speed of applications - part II</title><link href="http://markos.gaivo.net/articles/latency-bandwidth-and-speed-of-applications-part-ii.html" rel="alternate"></link><published>2005-09-16T00:03:00+02:00</published><updated>2005-09-16T00:03:00+02:00</updated><author><name>markos</name></author><id>tag:markos.gaivo.net,2005-09-16:/articles/latency-bandwidth-and-speed-of-applications-part-ii.html</id><summary type="html">&lt;div&gt;
 &lt;p&gt;
  This is the second post on this subject. You can find the first part
  &lt;a href="latency-bandwidth-and-speed-of-applications-part-i.html" title="First par of this article"&gt;
   here
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  So, how do latency and bandwidth relate to the speed of execution?
 &lt;/p&gt;
 &lt;p&gt;
  As mentioned, latency is time needed for any amount of information to travel a certain distance and bandwidth is the amount of data …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div&gt;
 &lt;p&gt;
  This is the second post on this subject. You can find the first part
  &lt;a href="latency-bandwidth-and-speed-of-applications-part-i.html" title="First par of this article"&gt;
   here
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  So, how do latency and bandwidth relate to the speed of execution?
 &lt;/p&gt;
 &lt;p&gt;
  As mentioned, latency is time needed for any amount of information to travel a certain distance and bandwidth is the amount of data that can flow through it at the same time.
 &lt;/p&gt;
 &lt;p&gt;
  Let me give you a real-life example. If you imagine people walking down the corridor, then latency is amount of time one person needs to walk from start to its end and bandwidth is number of people who can do this at the same time.
 &lt;/p&gt;
 &lt;p&gt;
  You can measure latency quite easily with ping, a tool available on every major OS, which will give you a round-trip time from client to server (2+5 from our original 7 intervals). The only problem is that it will tell you just your personal latency at that time, which can vary significantly for people on different networks and even for you at different times. There are similar tools for measuring bandwidth and they have more or less the same problems.
 &lt;/p&gt;
 &lt;p&gt;
  However, sometimes you can tell a bit about what you can expect. For example, if you’re building an application aimed at mobile users using GPRS, then it’s quite reasonable to assume that bandwidth will be scarce and latency will be high. On local networks it’s usually the opposite of that.
 &lt;/p&gt;
 &lt;p&gt;
  In general, latency rises with distance between server and client and bandwidth can’t be wider than the narrowest bandwidth on the way and the question is how do we react to our situation?
 &lt;/p&gt;
 &lt;p&gt;
  Basic rules are:
 &lt;/p&gt;
 &lt;ul&gt;
  &lt;li&gt;
   if latency is high, then we should make as few round-trips to server as possible
  &lt;/li&gt;
  &lt;li&gt;
   if bandwidth is wide, then we can transfer more data at each trip
  &lt;/li&gt;
 &lt;/ul&gt;
 &lt;p&gt;
  In practice we should always try for the most balanced approach between this two factors. If latency is fairly high, but bandwidth is not severely limited, then we might transfer also data we don’t need immediately, if it might spare us another request later on. If latency is low and bandwidth is narrow, then it might make more sense to transfer only what’s needed, but make those requests more often.
 &lt;/p&gt;
 &lt;p&gt;
  Another way we can sometimes cut down response time, when bandwidth is not the limiting factor, is to multiplex connections, which is a bit fancy way of saying to simultaneously open multiple connections from client to server. It’s nice to keep in mind that some browsers like Internet Explorer will limit number of simultaneous connections for HTTP protocol to 4 (as specified by its standard), but you are more free to do what you want, if you write your own program. It’s also nice to keep in mind that if order in which responses are coming is important, you’ll have to handle that yourself.
 &lt;/p&gt;
 &lt;p&gt;
  A different approach is to simulate multiplex connections over one connection. In this case client sends multiple commands inside of one network request and receives answers to all of them at once. It’s easier to handle order, since you get all data at the same time, but you give up possibly faster response for some of the data.
 &lt;/p&gt;
 &lt;p&gt;
  There are other factors that can influence our decisions, such as possibly increased load on server from more requests. Or the speed with which we can generate a request and process reply, if we need to pack and unpack more data. But this are issues worth exploring in some other post, if interest justifies it.
 &lt;/p&gt;
&lt;/div&gt;</content></entry><entry><title>Latency, bandwidth and speed of applications - part I</title><link href="http://markos.gaivo.net/articles/latency-bandwidth-and-speed-of-applications-part-i.html" rel="alternate"></link><published>2005-09-14T21:10:00+02:00</published><updated>2005-09-14T21:10:00+02:00</updated><author><name>markos</name></author><id>tag:markos.gaivo.net,2005-09-14:/articles/latency-bandwidth-and-speed-of-applications-part-i.html</id><summary type="html">&lt;div&gt;
 &lt;p&gt;
  It seems we’re back to discussing the importance of bandwidth and latency on perceived speed of application. In a way it’s amazing that after 10 years of widespread use of Internet and local networks, we developers still discuss these issues.
 &lt;/p&gt;
 &lt;p&gt;
  So, what contributes to speed of execution in …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div&gt;
 &lt;p&gt;
  It seems we’re back to discussing the importance of bandwidth and latency on perceived speed of application. In a way it’s amazing that after 10 years of widespread use of Internet and local networks, we developers still discuss these issues.
 &lt;/p&gt;
 &lt;p&gt;
  So, what contributes to speed of execution in network applications?
 &lt;/p&gt;
 &lt;p&gt;
  Each network request can be divided into seven time intervals:
 &lt;/p&gt;
 &lt;ol&gt;
  &lt;li&gt;
   time needed to send a request
  &lt;/li&gt;
  &lt;li&gt;
   latency, which is time spent for any amount of data to travel between client and server
  &lt;/li&gt;
  &lt;li&gt;
   data transfer time
  &lt;/li&gt;
  &lt;li&gt;
   time needed to process a request, form a response and send it
  &lt;/li&gt;
  &lt;li&gt;
   latency again, but this time in other direction
  &lt;/li&gt;
  &lt;li&gt;
   data transfer time for response
  &lt;/li&gt;
  &lt;li&gt;
   time needed to process response and present result to user
  &lt;/li&gt;
 &lt;/ol&gt;
 &lt;p&gt;
  Sum of all these parts is time lapsed between issued command and presented result. How fast this needs to be depends on application and our experiences. For example, we expect faster response to our key presses in SSH client than we do waiting for search results.
 &lt;/p&gt;
 &lt;p&gt;
  However, if this sum is under a quarter of a second, it will be generally regarded as instantaneous, but you really should aim for 1/10th of a second to satisfy even the most twitchy users.
 &lt;/p&gt;
 &lt;p&gt;
  Usually, we might get to control all parts of this equation only when client and server are located on a local network and we develop both of them. So, a well behaving application needs to take in account environment in which it will run and act accordingly.
 &lt;/p&gt;
 &lt;p&gt;
  This means, among other things, to give user feedback that something is happening whenever there is even a remote chance that response might take a while, to react sensibly to network disruptions (such as timeouts) and offer a way to abort user actions.
 &lt;/p&gt;
 &lt;p&gt;
  So, how do we speed up the application itself?
 &lt;/p&gt;
 &lt;p&gt;
  Absolutely the best way to speed up a network request is to not make one. The usual way to do this is to cache result over its life span and use it when possible.
 &lt;/p&gt;
 &lt;p&gt;
  The other way, lately often mentioned in relation to AJAX, is to make our requests asynchronously. Unlike synchronous requests, which are direct results of user actions, asynchronous are those were data is transfered in expectation of future user actions.
 &lt;/p&gt;
 &lt;p&gt;
  An example would be a mail program where we transfer message headers of yet unread mail in background before we actually need to display them to user, since it’s fairly likely that he’ll want to check them in near future.
 &lt;/p&gt;
 &lt;p&gt;
  There are also downsides to this approach. It can be difficult to predict what to transfer next if no user action is more probable than rest. It can also incur significant costs if bandwidth is expensive as often is on GPRS networks in Europe. And when number of simultaneous connections is limited (HTTP allows 4, but not all browsers comply), it can occupy a resource when you need one.
 &lt;/p&gt;
 &lt;p&gt;
  Still, it’s a useful tool when it can be applied.
 &lt;/p&gt;
 &lt;p&gt;
  More about saving time with our seven intervals in part two due tomorrow.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   Update: second part has been
   &lt;a href="latency-bandwidth-and-speed-of-applications-part-ii.html" title="Second part"&gt;
    posted
   &lt;/a&gt;
   .
  &lt;/em&gt;
 &lt;/p&gt;
&lt;/div&gt;</content></entry></feed>