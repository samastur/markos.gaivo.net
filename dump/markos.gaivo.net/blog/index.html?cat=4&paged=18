<!DOCTYPE html>
<html lang="en">
<head>
	<title>A notch above a monkey &raquo; Web</title>
	<meta charset="utf-8" />
	
	<meta name="viewport" content="width=device-width, initial-scale=1.0" /> 

	<link rel="shortcut icon" href="/img/markos.ico" />
	
	<meta name="generator" content="WordPress 3.0.5" /> <!-- leave this for stats please -->

	<link href="http://fast.fonts.com/cssapi/aee9b60b-8e94-47f8-b586-3ecbfab51d1c.css" rel="stylesheet" type="text/css" />
	<link rel="stylesheet" type="text/css" href="css/home.css" media="screen" title="Light theme" />
	<link rel="alternate stylesheet" type="text/css" href="css/home_dark.css" media="screen" title="Dark theme" />
	<link rel="stylesheet" href="css/ipad.css" media="only screen and (min-device-width : 768px) and (max-device-width : 1024px)">
	<link rel="stylesheet" type="text/css" href="css/smallscreen.css" media="only screen and (max-width: 700px)" />

	<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="http://markos.gaivo.net/blog/?feed=rss2" />
	<link rel="alternate" type="application/atom+xml" title="Atom 1.0" href="http://markos.gaivo.net/blog/?feed=atom" />
	<link rel='canonical' href='http://markos.gaivo.net/blog/?p=664' />

	<link rel="pingback" href="http://markos.gaivo.net/blog/xmlrpc.php" />

	<link rel="alternate" type="application/rss+xml" title="A notch above a monkey &raquo; Web Category Feed" href="http://markos.gaivo.net/blog/?feed=rss2&amp;cat=4" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://markos.gaivo.net/blog/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://markos.gaivo.net/blog/wp-includes/wlwmanifest.xml" /> 
<link rel='index' title='A notch above a monkey' href='http://markos.gaivo.net/blog' />
<meta name="generator" content="WordPress 3.0.5" />

	<!--[if lt IE 9]>
	<script src="/js/html5.js"></script>
	<![endif]-->
</head>
<body>
	<section class="envelope posts">
		<header>
			<h1><a href="http://markos.gaivo.net/blog/">A notch above a monkey</a></h1>
		</header>
		<nav class="colophon">
			<h1>By Marko Samastur</h1>
			<ul>
				<li><a href="/">About me</a></li>
				<li><a href="http://markos.gaivo.net/blog/?feed=atom">Subscribe</a></li>
			</ul>
		</nav>

	
		<article>
			<h1 class="storytitle"><a href="http://markos.gaivo.net/blog/?p=211" rel="bookmark">Speeding up Levenshtein</a></h1>

			<div class="post-text">
				<p><a href="http://markos.gaivo.net/blog/?p=210#postcomment">Simon</a> proposes using a dictionary to match mistyped URLs with real ones. I&#8217;d probably like the idea better if I actually understood it. Still using <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" title="description of the algorithm">Levenshtein</a> can be a bit easier than using a spell checker responsively.</p>
<p>Easier, but slow. I used existing <a href="http://hetland.org/python/distance.py" title="python implementation">implementation</a> by Magnus Lie Hetland and made a test with 245 blog titles using a simple <a href="http://markos.gaivo.net/examples/distance/test_distance" title="Link to test script">script</a>. 100 iterations on aging powerbook produced:</p>
<p style="text-indent:20pt;">1.766s, 29.152s, 9.399s (min, max, avg)</p>
<p>Average time to calculate distance between randomly chosen title and rest of them is 9.4 seconds, which is far too much to be useful. And there&#8217;s not even 250 of them.</p>
<p>There are two obvious ways to speed things up. Since minimum distance is at least as much as a difference in length of both strings, there&#8217;s no point in calculating it when difference already exceeds a limit we chose.</p>
<p>The other trick took into an account that Levenshtein&#8217;s algorithm of two strings of comparable length has complexity of O(n2) and that my blog titles are form quite sparse space. If strings are longer than a certain limit (I arbitrarily chose 10 letters), then first calculate edit distance on a small sparse substring and then calculate the real distance only if first one was close enough.</p>
<p>When I made 1000 iterations of randomly chosen title using only first test, I got following results:</p>
<p style="text-indent:20pt;">0.003s, 0.284s, 0.167s (min, max, avg)</p>
<p>However, if I used both checks, the same 1000 iteration test got me:</p>
<p style="text-indent:20pt;">0.003s, 0.162s, 0.027s (min, max, avg)</p>
<p>So, <a href="http://markos.gaivo.net/examples/distance/distance.py" title="Extended distance functions">two simple checks</a> can speed up search up to 350 times. Not bad, but I&#8217;m not happy. This is certainly fast enough for a personal website or sites where site structure effectively divides searching to relatively small sets of possible hits, but there are plenty of sites where it would be too slow.</p>
<p>I tried to simplify searching using distance to map strings to coordinate system, which was at least hopelessly naive if not downright dumb. A much better approach would be to read more, which is what I&#8217;m doing now.</p>
<p>By the way, I&#8217;ve also tried to speed it up using Pyrex, but got pretty much same results, which means I either don&#8217;t know how to use Pyrex or Python optimizes well. Or both.</p>
			</div>

			<footer class="post-metadata">
				<ul>
					<li>Published on <time datetime="2006-11-10T02:17:48+00:00">November 10, 2006</time></li>
					<li><a href="http://markos.gaivo.net/blog/?p=211#comments" title="Comment on Speeding up Levenshtein">Add your comment</a> </li>
				</ul>
			</footer>
		</article>
	
		<article>
			<h1 class="storytitle"><a href="http://markos.gaivo.net/blog/?p=210" rel="bookmark">Handling 404</a></h1>

			<div class="post-text">
				<p>This blog doesn&#8217;t use descriptive URLs, which is not the only time I screwed this up for no good reason. In this case it was mainly laziness and smug i-don&#8217;t-care-if-people-find-me attitude, but I hadn&#8217;t really realized how stupid this decision was until I started thinking about the problem of missing pages (error 404).</p>
<p>It always bugged me how useless most 404 pages are. Sure, I could notify the webmaster about a broken link, but that won&#8217;t help me find what I&#8217;m looking for. Can&#8217;t we do better than that?</p>
<p>As it happens we often can.  There&#8217;s an unlimited number of ways in which visitors can fail to reach their destination, but majority of them probably fall in four categories:</p>
<ul>
<li>content moved elsewhere</li>
<li>broken links</li>
<li>typos</li>
<li>bad memory recall</li>
</ul>
<p><strong>Content moved elsewhere</strong></p>
<p>Many believe that pages should be permanent and links shouldn&#8217;t break over time. Yet sometimes we either have little control over this or there are good reasons for breakage. We should still try to mitigate such situation by guiding visitors to correct new location when possible.</p>
<p>This is done with HTTP redirects. If content was moved only temporary, then response should have a status code of 302 and contain a link in the header to correct current address. If move was permanent, then the same thing is achieved with code 301.</p>
<p>Missing pages amount to around 0.6% of hits on this website. They would be around 8% if redirects weren&#8217;t used.</p>
<p><strong>Broken links</strong></p>
<p>Broken links appear when email clients break URL longer than maximum line length or from botched copy operation. This usually means that address is cut off and part we have is incomplete, but otherwise correct. Handling such links can range from trivial to impossible. Let&#8217;s look at one Wikipedia link as an example:</p>
<p style="text-indent:20pt;"><a href="http://en.wikipedia.org/wiki/Longest_common_subsequence_problem" title="Link to article about longest common sequence problem">http://en.wikipedia.org/wiki/Longest_common_subsequence_problem</a></p>
<p>Let&#8217;s say that link was broken near the end of it and there was <em>lem</em> missing in <em>problem</em>. Resulting address might not be complete, but there probably aren&#8217;t that many Wikipedia articles where such string forms first part of their address and it would be reasonable to assume that Wikipedia could find the right article. Alas it doesn&#8217;t.</p>
<p>On the other side of the spectrum are impossible or almost impossible guesses. If address was broken anywhere before <em>Longest</em>, then we could learn nothing from it about visitors expectations. This would look like a good place to give up.</p>
<p>However, if we are lucky, then our visitors came from one of popular search engines, which means their referrer attribute includes search string that led to our page. We can extract those keywords, use them and hopefully find that page or failing that offer a list of related pages. Not perfect, but beats plain &#8220;Not here&#8221; sign.</p>
<p><strong>Typos</p>
<p></strong>Typos are what happens when people use keyboards. I can&#8217;t live without one, but I still recognize that my fingers and my brain are not always of the same mind about how to spell a word. Letters get added, dropped or just switch places, all being a problem for a program that usually looks for that one perfect match.</p>
<p>There&#8217;s help. Edit distance algorithms, like <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" title="Description of Levenshteins algorithm">Levenshtein&#8217;s</a>, can be used to measure how similar two strings really are. Matching then becomes finding the page with shortest distance from a list of its closest neighbors.</p>
<p>Downside is that algorithm is fairly computationally expensive and it might take time to find a match. I&#8217;ll explore this problem in tomorrows article.</p>
<p><strong>Bad memory requests</strong></p>
<p>The main purpose of descriptive URLs is the same as catchy domain names. Make it easy to remember address for later use, when bookmarks or browsers autocomplete can&#8217;t be used.</p>
<p>But memory is notoriously unreliable and it doesn&#8217;t work any better with web addresses. So addresses used may vary enough from the right one that they don&#8217;t get caught with edit distance algorithms. As an example, someone might try to access aforementioned Wikipedia&#8217;s article with:</p>
<p style="text-indent:20pt;">http://en.wikipedia.org/wiki/Longest_subsequence_problem</p>
<p>Calculated distance between this address and the real one is 7, which is probably more than would be real limit for matching. We can still look at referrer for clues, but we can also use requested address. In our case, we can extract keywords <em>longest</em>, <em>subsequence</em> and <em>problem</em> and use them to search for possible hits. Wikipedia doesn&#8217;t do this either, but neither do I, so I shouldn&#8217;t complain.<br />
<strong>Time to wrap it up</strong></p>
<p>I believe this approaches make a good case for logical and descriptive addresses, since most of them don&#8217;t work (well) otherwise. If someone requests an article with a nonexistent ID 145, it&#8217;s impossible to resolve which of existing ones with IDs 155, 245 or 149 he really wanted.</p>
<p>Still, sometimes descriptive addresses are not an option. I&#8217;d love to hear ideas or practical experience of how to handle such cases.</p>
			</div>

			<footer class="post-metadata">
				<ul>
					<li>Published on <time datetime="2006-11-08T12:31:43+00:00">November 8, 2006</time></li>
					<li><a href="http://markos.gaivo.net/blog/?p=210#comments" title="Comment on Handling 404">Add your comment</a> </li>
				</ul>
			</footer>
		</article>
	
		<article>
			<h1 class="storytitle"><a href="http://markos.gaivo.net/blog/?p=209" rel="bookmark">Preventing download of javascript on mobile web</a></h1>

			<div class="post-text">
				<p>Every year I spend a few weeks somewhere where internet connection is either slow or metered and expensive. Usually it&#8217;s both which makes me rather twitchy when it comes to big web page sizes. Yet I&#8217;m also certain that my new web home, however it may turn out at the end, will have a significant chunk of it written in Javascript. None of it necessary for working, but more than I&#8217;d probably like to download over flimsy mobile phone connection.</p>
<p>The problem are not really old mobile browsers which don&#8217;t support Javascript. They won&#8217;t download any of it anyway. The question is how to prevent eager browsers, who would, from downloading this stuff when you don&#8217;t want them to. My first, rather primitive attempt was <a href="http://markos.gaivo.net/examples/envcheck/index.html" title="Demo of javascript ondemand loading depending on media">this demonstration</a>, which only works in Opera 9, Safari and Firefox, but most certainly doesn&#8217;t work in all browsers.</p>
<p>What it does is check font size of a title and based on its value resolves which media style sheet was used. If it was <em>mobile.css</em>, which is used only when media is set to <em>handheld</em>, then it was probably done from a mobile environment, so it includes the mobile version of a Javascript or it could, if I wanted, none at all.</p>
<p>There are several problems with this approach. First one is that it doesn&#8217;t recognize notebook users connecting over a  cellphone. It can&#8217;t really, since browser environment is literally the same, unless it would try to measure latency and bandwidth of page elements and guess from those results, which is neither easy nor reliable. 3G networks can be rather fast and have a better latency than a wired connection from somewhere like Tanzania.</p>
<p>But let&#8217;s say we don&#8217;t care about this case. We can always turn Javascript off in Firefox if it&#8217;s important enough to us, which leads us to the next problem. Support for handheld media type is still rather spotty. If browser doesn&#8217;t support it, it may load the wrong style sheet if any at all and the wrong CSS value results in wrong turn in Javascript. However <em>handheld media</em> support is getting better. Since I decided from the start that my personal page is a good place where leading edge can also be a bit of bleeding one, that is good enough. On a different site this probably wouldn&#8217;t be true.</p>
<p>So it sort of works in principle, but it is crude and error prone. Javascript check is not self-contained and I could easily break it by changing font size value of a title in CSS files while forgetting to do a corresponding correction in code. What would be much better is to learn from the browser which media types style sheets were actually used and act accordingly. Now that would be grand.</p>
<p>There are two ways you could go about this. My first go was finding style nodes in DOM and looking at their <em>disabled</em> property, which is commonly used in style sheet switchers for turning sheets on and off. It doesn&#8217;t work, since &#8216;wrong&#8217; media types in Firefox are ignored, not disabled. Their <em>disabled</em> value is still set to <em>false</em>.</p>
<p>A proper way of doing it would be using DOM Style Sheets methods. Basic idea is to compare actual values as set in the page with values read from style sheets and resolving which ones were used. While not exactly trivial, by little forethought it can be made to work fine in Opera and Firefox. It can also work in Explorer using its own methods, but it&#8217;s a pain to make it  work in Safari, which in this case is not only an incompetent liar, it&#8217;s also lying in some weird accent. If you&#8217;d like to learn more about it, then there&#8217;s a <a href="http://www.howtocreate.co.uk/tutorials/javascript/domstylesheets" title="Problems with DOM Style Sheets use">great</a> page describing current state but the gist of it is that you can&#8217;t rely on methods being there and even when they are, in some form or another, they might not work reliably (Safari) or might produce weird results (yup, Safari again).</p>
<p>Well, it could still work, but would require an unreasonable amount of code. Hence I made a different tradeoff, which can be seen <a href="http://markos.gaivo.net/examples/envcheck/index2.html" title="Second attempt">here</a>. A bit less automation for a lot less code. It keeps the idea of first demo with a small twist. First rule of every media style sheet targets the same element that gets created by Javascript if necessary and compares its font size value with those found in style sheets. It returns media types of all sheets where this value was found.</p>
<p>The end result certainly isn&#8217;t perfect or pretty, but it keeps amount of bookkeeping to minimum and limited to CSS files. Even using a &#8220;safe&#8221; property like font size can lead to problems (e.g. 0.9em is sometimes interpreted as 0.90em), but nothing difficult to overcome.</p>
<p>It might not be of production quality, but it will work as good starting point for further exploration.</p>
			</div>

			<footer class="post-metadata">
				<ul>
					<li>Published on <time datetime="2006-10-29T21:55:26+00:00">October 29, 2006</time></li>
					<li><a href="http://markos.gaivo.net/blog/?p=209#comments" title="Comment on Preventing download of javascript on mobile web">Add your comment</a> </li>
				</ul>
			</footer>
		</article>
		
		<nav class="pagination">
			<ul>
				<li><a href="http://markos.gaivo.net/blog/?cat=4&#038;paged=17" >&laquo; Newer articles</a></li>
				<li><a href="http://markos.gaivo.net/blog/?cat=4&#038;paged=19" >Older articles &raquo;</a></li>
			</ul>
		</nav>
	</section>


	<footer>
		<ul>
			<li>&copy; 2005-2015 Marko Samastur</li>
			
			<li><a href="http://markos.gaivo.net/blog/?feed=rss2">Entries feed</a></li>
			<li><a href="http://markos.gaivo.net/blog/?feed=comments-rss2">Comments feed</a></li>
		</ul>
		<a href="/"><img src="/img/markos_light_small_bg.png" alt="logo" class="mini-logo" /></a>
	</footer>
	<script src="/js/jquery.js"></script>
	<script src="/js/all.js"></script>
	<!--script src="/mint/?js" type="text/javascript"></script-->

	<!-- Google analytics -->
	<script type="text/javascript">

	  var _gaq = _gaq || [];
	  _gaq.push(['_setAccount', 'UA-152438-1']);
	  _gaq.push(['_trackPageview']);
	</script>
	<script src="/js/cookie.js"></script>
</body>
</html>
