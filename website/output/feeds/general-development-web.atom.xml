<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A notch above a monkey</title><link href="/" rel="alternate"></link><link href="/articles/feeds/general-development-web.atom.xml" rel="self"></link><id>/</id><updated>2012-03-15T08:52:00+01:00</updated><entry><title>Save bandwidth switch</title><link href="/save-bandwidth-switch.html" rel="alternate"></link><updated>2012-03-15T08:52:00+01:00</updated><author><name>markos</name></author><id>tag:,2012-03-15:save-bandwidth-switch.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Michal Migurski recently posted
    &lt;a href="http://mike.teczno.com/notes/bandwidth.html"&gt;
     an article
    &lt;/a&gt;
    about download sizes of popular websites. I couldn’t replicate his results
    &lt;sup&gt;
     &lt;a href="#save-bandwidth-note-1" id="save-bandwidth-1"&gt;
      [1]
     &lt;/a&gt;
    &lt;/sup&gt;
    , but it is obvious that gist of Michal’s article is correct, websites have indeed ballooned significantly in last few years.
   &lt;/p&gt;
   &lt;p&gt;
    This blog’s homepage has a footprint of around 250KB-270KB
    &lt;sup&gt;
     &lt;a href="#save-bandwidth-note-2" id="save-bandwidth-2"&gt;
      [2]
     &lt;/a&gt;
    &lt;/sup&gt;
    . About 90% of its size are fonts and
    &lt;a href="http://jquery.com/"&gt;
     jQuery
    &lt;/a&gt;
    which is a big penalty for making it look and behave a bit nicer. So should I remove those parts?
   &lt;/p&gt;
   &lt;p&gt;
    Well, for most visitors to this website that difference doesn’t matter. Pages for them are neither slow nor expensive to load. Unless of course they are doing it over your average hotel Wi-Fi or a slow mobile network where speed around 56Kb/s is not unheard of. On such connection it would take about half a minute to load this blog. It can also cost more than 10 euro cents to load it when roaming in Europe.
   &lt;/p&gt;
   &lt;p&gt;
    It would be great if I could offer a choice of serving bigger and nicer or smaller and faster version depending on visitors needs.
   &lt;/p&gt;
   &lt;p&gt;
    Measuring speed is not easy, but certainly doable as Gmail has demonstrated. You could start a timer immediately in page header, measure how much time it takes to load a smaller version of a website and if that happened quickly enough upgrade it to full bling. Android browser also added support for
    &lt;a href="http://dvcs.w3.org/hg/dap/raw-file/tip/network-api/index.html"&gt;
     navigator.connection
    &lt;/a&gt;
    Javascript property which, where it exists, probably has more details than you would need.
   &lt;/p&gt;
   &lt;p&gt;
    However there is no way to measure price of a visit. Even if I could, how would I decide what is too expensive for an anonymous reader and should I make such decisions at all? I think not.
   &lt;/p&gt;
   &lt;p&gt;
    Gmail’s approach is really just a band-aid over what should be a visitor’s decision. I use same laptop and browser at home and while I travel, experiencing all combinations of connection speed and pricing. I never know how much it will cost me to visit a page, but I always learn quickly if I would prefer something small or full-featured. There is just no way I can communicate that preference.
   &lt;/p&gt;
   &lt;p&gt;
    It would be great if my browser had a switch for this purpose, like Firefox’s “Work Offline” toggle. So if I switched to bandwidth saving mode, then every subsequent request to web server would communicate my preference with a HTTP header field like:
   &lt;/p&gt;
   &lt;blockquote&gt;
    &lt;p&gt;
     X-Bandwidth: save
    &lt;/p&gt;
   &lt;/blockquote&gt;
   &lt;p&gt;
    In principle you could have multiple levels of bandwidth consumption, but that would likely be an overkill. Common practice suggests that at most two levels would really get used, one aimed at mobile devices and other at desktop.
   &lt;/p&gt;
   &lt;p&gt;
    Header like that might be enough, but even better would be if Javascript environment got another property describing current state of user’s preference (like say
    &lt;em&gt;
     navigator.bandwidth
    &lt;/em&gt;
    ). Coupled with a bandwidth event triggered on change you could really optimize every modern web page, even those with more complicated loading of resources and execution paths.
   &lt;/p&gt;
   &lt;p&gt;
    Right now such functionality doesn’t exist or at least I couldn’t find it (I even searched Mozilla’s bug database for any future plans). I think my proposal is both user and developer friendly and workable. If you can think of a reason why it would be problematic, then I would really like to hear it.
   &lt;/p&gt;
   &lt;ol&gt;
    &lt;li id="save-bandwidth-note-1"&gt;
     Pages are often personalized for visitor. Developer tools of different browsers also don’t report same sizes. They also report amount of data transferred not the size of that data once unpacked. Almost 2M of Twitter’s Javascript is thus reduced into page of “only” about 1MB of data transferred. And that to display couple of sentences.
     &lt;a href="#save-bandwidth-1"&gt;
      ↩
     &lt;/a&gt;
    &lt;/li&gt;
    &lt;li id="save-bandwidth-note-2"&gt;
     Depends on browser used. Variation in sizes is probably due different formats of fonts used by browsers. It also changed once I published this post.
     &lt;a href="#save-bandwidth-2"&gt;
      ↩
     &lt;/a&gt;
    &lt;/li&gt;
   &lt;/ol&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>-beta- prefix algorithm for CSS</title><link href="/beta-prefix-algorithm-for-css.html" rel="alternate"></link><updated>2012-02-10T18:21:00+01:00</updated><author><name>markos</name></author><id>tag:,2012-02-10:beta-prefix-algorithm-for-css.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    These have been eventful couple of days for web developers. CSS Working Group chair
    &lt;a href="http://www.glazman.org/weblog/dotclear/index.php?post/2012/02/09/CALL-FOR-ACTION%3A-THE-OPEN-WEB-NEEDS-YOU-NOW"&gt;
     called on everyone
    &lt;/a&gt;
    to use all (most?) vendor prefixes and stop making websites for WebKit which is becoming a new (mobile) IE6. Responses have been numerous,
    &lt;a href="http://www.quirksmode.org/blog/archives/2012/02/the_vendor_pref.html" title="PPK's first article about prefixes"&gt;
     including
    &lt;/a&gt;
    &lt;a href="http://www.quirksmode.org/blog/archives/2012/02/alpha_and_beta.html" title="PPK's follow up to first article"&gt;
     ppk
    &lt;/a&gt;
    who in his usual obnoxious manner
    &lt;sup&gt;
     &lt;a href="#beta-prefix-note-1" id="beta-prefix-1"&gt;
      [1]
     &lt;/a&gt;
    &lt;/sup&gt;
    made some good points. Testing on mobile devices is an unsolved problem (who wants or can afford to buy so many almost immediately obsolete gadgets?) and introducing -beta- (maybe also -alpha-) prefix would simplify our lives while keeping most benefits of vendor prefixes.
   &lt;/p&gt;
   &lt;p&gt;
    I like -beta- idea and I think adding -alpha- might be even better. There’s still a problem of resolving syntax conflicts between different implementation which I think has a simple solution that closely mimics what browsers already do:
   &lt;/p&gt;
   &lt;p&gt;
    &lt;em&gt;
     When parsing CSS browsers
    &lt;/em&gt;
    &lt;em&gt;
     should apply the last matching -beta-/-alpha- directive they fully understand.
    &lt;/em&gt;
   &lt;/p&gt;
   &lt;p&gt;
    Browsers already ignore directives they don’t understand and they apply last directive found when there are multiple candidates for a DOM node.
   &lt;/p&gt;
   &lt;p&gt;
    Such behavior would give us less CSS code to write and maintain, have predictable behavior and keep browser experimentation without favoring one. I have troubles finding negative sides of this approach, but do let me know if you can think of one.
   &lt;/p&gt;
   &lt;ol&gt;
    &lt;li id="beta-prefix-note-1"&gt;
     I deeply dislike his complaining about simplistic view of others while himself generalizing and name-calling (the lazy and stupid lot of us). Alas it’s not good to read only people you like and agree with.
     &lt;a href="#beta-prefix-1"&gt;
      ↩
     &lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     Almost everything I write on this blog has an intended  audience of one:  me (no, really!). Why I sometimes write posts like  this, which don’t, is a mistery  since their expected and actual effect on anyone  is…none.
    &lt;/li&gt;
   &lt;/ol&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Cookies, localStorage and shared state</title><link href="/cookies-localstorage-and-shared-state.html" rel="alternate"></link><updated>2011-12-19T17:05:00+01:00</updated><author><name>markos</name></author><id>tag:,2011-12-19:cookies-localstorage-and-shared-state.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I’ve been fiddling with my website again, changing theme switching from somewhat dumb class based system to a more proper one using alternate style sheets. I learned that picking a style sheet in browser applies those changes only to currently open page, so for style sheet selection to persevere it needs a bit of Javascript support from website owner. Personally I find this just stupid.
   &lt;/p&gt;
   &lt;p&gt;
    The easiest way to remember visitor’s preference is to store it in his browser. Cookies used to be popular before they were deemed evil, but they have other limitations as well. Hence popular switch to HTML5 in-browser storage technologies like
    &lt;a href="http://www.w3.org/TR/webstorage/"&gt;
     localStorage
    &lt;/a&gt;
    .
   &lt;/p&gt;
   &lt;p&gt;
    I think there is one important difference between cookies and tools like localStorage that is often overlooked and it’s not the size of data that can be stored. Cookies are sent with
    &lt;em&gt;
     each page request
    &lt;/em&gt;
    while data stored elsewhere isn’t. Changing them on any side will automatically share state with the other. I use localStorage in my theme switcher because I think server doesn’t need and should not know which theme is used. But for storing shared data, especially one that expires, cookies remain a reasonable if not best choice.
   &lt;/p&gt;
   &lt;p&gt;
    None of this is exactly new, but I think it is worth remembering. In other news I dislike interface limitations of Chrome more and more (exceptions are Developer Tools and extensions framework).
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Reading sources</title><link href="/reading-sources.html" rel="alternate"></link><updated>2011-11-21T11:37:00+01:00</updated><author><name>markos</name></author><id>tag:,2011-11-21:reading-sources.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Google Reader was redesigned lately and I’ve been annoyed ever since. I had a dubious privilege of cutting and changing product features people loved in pursuit of
    &lt;del&gt;
     higher
    &lt;/del&gt;
    different goals, so I try to be understanding when others do the same. I mostly found clumsy workarounds for removed features, but I do wish I could at least still trust that list of unread items actually has
    &lt;em&gt;
     all of them
    &lt;/em&gt;
    . On a positive note I can save some electricity now because copious amounts of “helpfully” white whitespace illuminate this room brightly enough that you wouldn’t sit naked in-front of your computer even with lights turned off. That is if you are the sort of person who likes doing that but stops short of flashing your neighbors.
   &lt;/p&gt;
   &lt;p&gt;
    I still strongly dislike changes made, but I continue using Google Reader, because crack-heads don’t give up dope just because it was cut too thinly. I cherish my list of reading sources and like a gardener I have been cultivating it through years because I believe they make me better informed than I would be if I relied only on links shared by others. This may be elitist, but it is also true.
   &lt;/p&gt;
   &lt;p&gt;
    We are biased when choosing friends and communities we belong to. At the very least we enjoy our life more when surrounded with like minded people which is really a lighter shade of group think. We share to tell stories as much about what is shared as we do about who we are. Even when not self-censoring or trying to project an image we still are horribly bad at evaluating what influences us and how. Sharing everything, as
    &lt;a href="http://techcrunch.com/2011/11/19/curation-through-unsharing/"&gt;
     this idiotic article
    &lt;/a&gt;
    suggests, doesn’t fix this
    &lt;sup&gt;
     &lt;a href="#reading-sources-note-1" id="reading-sources-1"&gt;
      [1]
     &lt;/a&gt;
    &lt;/sup&gt;
    . It’s still content from same people only more of it.
   &lt;/p&gt;
   &lt;p&gt;
    Then there are social new sites, which are in essence news organizations with bigger editorial board. Their focus might not be the same and their world view less obvious (or not) as of traditional boards, but the end result really isn’t all that different. I don’t dread waking up in a world without
    &lt;a href="http://www.apple.com" title="Apple's homepage"&gt;
     Apple
    &lt;/a&gt;
    as I do in one without  fish, but it is not articles about all things piscatorial that keep popping up on regular basis.
   &lt;/p&gt;
   &lt;p&gt;
    This doesn’t make socially filtered news useless, just limited and best suited for finding out what is popular at this moment. They should be a side dish not the whole diet. Getting some of your information diet from social sources may improve it, but relying only on them is just stupid. I wouldn’t fret so much if I didn’t worry about development trends — latest Reader changes being one example of them.
   &lt;/p&gt;
   &lt;p&gt;
    Reader had two methods of sharing. Obvious one was button Share which was adequately replaced with sharing to
    &lt;a href="http://plus.google.com"&gt;
     Google+
    &lt;/a&gt;
    circles. The other one, which was the one I actually relied on, was to create public feeds for articles marked with certain tags. The most important difference is that in first case you grouped by intended audience and in second by actual content
    &lt;sup&gt;
     &lt;a href="#reading-sources-note-2" id="reading-sources-2"&gt;
      [2]
     &lt;/a&gt;
    &lt;/sup&gt;
    . Instead of following me, you could just follow my selection on particular topic which in most cases would probably be closer to what you want.
   &lt;/p&gt;
   &lt;p&gt;
    By itself stripping a feature like that doesn’t mean much. However when I also judge other changes such as aforementioned abundance of whitespace, removal of “Note to reader” and  new reading unfriendly theme, it’s easy to come to conclusion that all roads now lead to Google+. Reader’s role is at best to feed its younger brother with stuff to socialize around.
   &lt;/p&gt;
   &lt;p&gt;
    It would be wrong to attribute these changes simply to competition with Facebook since they are a part of a larger trend to social curation. I find this trend just a normal consequence of a web ecosystem where most product innovation happens in VC funded startups. How companies were funded was always a part of their DNA and economics of today’s VC environment for companies that will probably be acquired at some point (and let’s be honest, who REALLY believes most news experiments won’t be?) almost demands a quick and high growth. It’s not impossible to achieve this with sources-based product, but it’s certainly harder and less obvious than creating another twist on social news.
   &lt;/p&gt;
   &lt;p&gt;
    If my first and main point was a personal appeal to seek insight also in your own, personally picked sources, then my second is to question if shaping web and world with it should really be left only to industry and academia. It really doesn’t have to be this way.
   &lt;/p&gt;
   &lt;ol&gt;
    &lt;li id="reading-sources-note-1"&gt;
     Browser’s history is a great place to see just how much of what we visit is unimportant, unrepresentative and often unsharable. A small friction necessary for a deliberate act of sharing is actually a feature that gives at least a modicum of reflection on content’s share-worthiness.
     &lt;a href="#reading-sources-1"&gt;
      ↩
     &lt;/a&gt;
    &lt;/li&gt;
    &lt;li id="reading-sources-note-2"&gt;
     Feeds enable that and are one of crucial building blocks for what I started to call
     &lt;em&gt;
      social software for introverts
     &lt;/em&gt;
     .  It is software which is better when used by many, but is good even when  you are its only user.
     &lt;a href="http://www.instapaper.com/" title="Tool for saving pages to read them later"&gt;
      Instapaper
     &lt;/a&gt;
     would be a perfect example of such an application and
     &lt;a href="http://www.facebook.com/" title="Evil empire"&gt;
      Facebook
     &lt;/a&gt;
     is a counter-example.
     &lt;a href="#reading-sources-2"&gt;
      ↩
     &lt;/a&gt;
    &lt;/li&gt;
   &lt;/ol&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Missing refer(r)ers</title><link href="/missing-referrers.html" rel="alternate"></link><updated>2011-09-20T18:29:00+02:00</updated><author><name>markos</name></author><id>tag:,2011-09-20:missing-referrers.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I was talking to my wife today about why an “Unknown Source” is becoming the most common source of visits on
    &lt;a class="zem_slink" href="http://flickr.com" rel="homepage" title="Flickr"&gt;
     Flickr
    &lt;/a&gt;
    . My guess is that it is almost certainly because a
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/HTTP_referrer" rel="wikipedia" title="HTTP referrer"&gt;
     &lt;em&gt;
      Referer
     &lt;/em&gt;
    &lt;/a&gt;
    header field is missing in page requests and Flickr is only relying on presence of this piece of data to discern sources. This might have made sense when browsers ruled the web, but we don’t live then anymore.
   &lt;/p&gt;
   &lt;p&gt;
    Simply put referrer field
    &lt;a href="http://tools.ietf.org/html/rfc2616#section-14.36" title="Link to Referer field definition"&gt;
     can be added
    &lt;/a&gt;
    to request only when request was triggered from source which also has an
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Uniform_Resource_Identifier" rel="wikipedia" title="Uniform Resource Identifier"&gt;
     URI
    &lt;/a&gt;
    address. Every web page has it since its address is also an URI, but most programs don’t.
   &lt;/p&gt;
   &lt;p&gt;
    That’s why referrers aren’t missing only from visitors using paranoid browsers or security software that stripped it out of requests, but also when page is visited from a link embedded in emails or instant messages.
   &lt;/p&gt;
   &lt;p&gt;
    Requests without referrers used to present a small enough subset of visits that most of us didn’t care. I first noticed this changing when Twitter clients became popular and now there are tons of apps behaving like this. With
    &lt;a href="http://www.w3.org/TR/XMLHttpRequest2/" title="Link to specification"&gt;
     XMLHttpRequest Level 2
    &lt;/a&gt;
    ﻿ even web developers will get a choice of making anonymous requests which won’t include potentially private data like referrers.
   &lt;/p&gt;
   &lt;p&gt;
    That’s nice. I have certainly visited places which I wouldn’t want to share with strangers. But I suspect this is not the common case and most of the time we don’t care telling which page or application sent us there.
   &lt;/p&gt;
   &lt;p&gt;
    Nobody can precisely predict future so specifications can never be perfect. But I am always amazed when reading core web specs like HTTP’s how insightful their authors were. That most programs don’t have a URI does not mean that they couldn’t. Practically anything can and definitely more things should.
   &lt;/p&gt;
   &lt;p&gt;
    In principle it is possible for each program to have its own address which could be used as referrer
    &lt;em&gt;
     when a better option doesn’t exist
    &lt;/em&gt;
    . I suspect this would be against the spirit of specification, but likely not against its text.
   &lt;/p&gt;
   &lt;p&gt;
    However better options often do exist. Lets take
    &lt;a class="zem_slink" href="http://twitter.com" rel="homepage" title="Twitter"&gt;
     Twitter
    &lt;/a&gt;
    as a popular example. Every link that triggers a visit from Twitter was included in at least one tweet and every tweet is also published on web. It might not be accessible to everyone, but it does have an address. I see no reason why that address could not be used as referrer by Twitter clients.
   &lt;/p&gt;
   &lt;p&gt;
    It is originating content or its address that interest me, but really, almost anything truthful beats an unknown source. Just knowing name of the service (like Twitter) or app used (e.g. Tweetdeck) would be better than nothing. By the way there is another header field that could provide such insight:
    &lt;em&gt;
     User-Agent
    &lt;/em&gt;
    . Sadly it is notoriously unreliable and often missing as well, but that’s another long story.
   &lt;/p&gt;
   &lt;p&gt;
    So, if I come back to Flickr. I can’t really tell how much Flickr can or could know. I suspect more than it tells, but I would be astonished if they are not mostly in the dark too. Web was largely built by people too busy (and often too lazy) not to cut corners. Meaning: learning and doing as little as possible to get something to work and sometimes we pay price for our ignorance. But it’s a small price compared to waiting for
    &lt;a href="http://en.wikipedia.org/wiki/Project_Xanadu" title="Description of project Xanadu on Wikipedia"&gt;
     Xanadu
    &lt;/a&gt;
    .
   &lt;/p&gt;
   &lt;div class="zemanta-pixie"&gt;
    &lt;img alt="" class="zemanta-pixie-img" src="http://img.zemanta.com/pixy.gif?x-id=860d2c48-a1dc-49c5-85d1-277e4734d096"/&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title></title><link href="/address.html" rel="alternate"></link><updated>2011-08-26T17:26:00+02:00</updated><author><name>markos</name></author><id>tag:,2011-08-26:address.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    After years of personal embarrassment I finally managed to update look and code of my website. This actually happened last week and I wanted to write this post sooner, but
    &lt;a href="http://supervizor.kpk-rs.si" title="Supervizor"&gt;
     other things
    &lt;/a&gt;
    came in-between. There are still bugs I need to fix (iPad footer is better than it was, but not fine), but﻿ overall I’m pretty happy with how it turned out.
   &lt;/p&gt;
   &lt;p&gt;
    This was a second site I built using
    &lt;a class="zem_slink" href="http://www.w3.org/TR/css3-mediaqueries/" rel="homepage" title="media queries"&gt;
     media queries
    &lt;/a&gt;
    and I learned lots. When I learn a bit more about mobile web, I might write a post about few gotchas for new developers, but this is not that post. This post is about
    &lt;code&gt;
     &amp;lt;address&amp;gt;
    &lt;/code&gt;
    tag because it bugged me when I made that first design and I spent a remarkable amount of time on it again.
   &lt;/p&gt;
   &lt;p&gt;
    My problem with it came down to its definition.
    &lt;code&gt;
     &amp;lt;address&amp;gt;
    &lt;/code&gt;
    can be used only to mark up contact information about content’s author. In HTML 4 this was limited to authorship of whole page. For HTML5 it
    &lt;a href="http://dev.w3.org/html5/spec/sections.html#the-address-element" title="Definition of &amp;lt;address&amp;gt; in HTML5 specification"&gt;
     was “widened”
    &lt;/a&gt;
    to authorship of sectioning element like
    &lt;code&gt;
     &amp;lt;article&amp;gt;
    &lt;/code&gt;
    (so if you have more than one article on page, each can contain its own, different, address for its author). You can’t however mark any address with it and after lots of searching I found
    &lt;a href="http://lists.whatwg.org/htdig.cgi/whatwg-whatwg.org/2008-February/014023.html" title="Ian's message from February 2008"&gt;
     this old message
    &lt;/a&gt;
    from
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Ian_Hickson" rel="wikipedia" title="Ian Hickson"&gt;
     Ian Hickson
    &lt;/a&gt;
    which explains why (I’m not aware of any later clarifications).
   &lt;/p&gt;
   &lt;p&gt;
    I don’t agree with everything said. Mostly correct use is not necessarily a proof of well-designed element. My bet would be that
    &lt;code&gt;
     &amp;lt;address&amp;gt;
    &lt;/code&gt;
    was a rather unknown tag exactly because of its limited use and was thus used mostly by those that dig deeper into standards, which sound also like people who care  how things are used as expected.
   &lt;/p&gt;
   &lt;p&gt;
    Still most points are valid. Loosening definition would make it less meaningful.
    &lt;a href="http://microformats.org/wiki/hcard" title="Specification of hCard microformat"&gt;
     There
    &lt;/a&gt;
    &lt;a href="http://microformats.org/wiki/adr" title="Specification of adr microformat"&gt;
     are
    &lt;/a&gt;
    microformats that you can use instead and most annoying problems (allowing only inline content) have been fixed. We also have better tools for parsing HTML so  it really doesn’t matter much if its definition feels completely right or not. It’s good enough.
   &lt;/p&gt;
   &lt;p&gt;
    At the end I still decided to mark my contact information on
    &lt;a href="http://markos.gaivo.net"&gt;
     my homepage
    &lt;/a&gt;
    with
    &lt;code&gt;
     &amp;lt;address&amp;gt;
    &lt;/code&gt;
    anyhow (augmented with
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/HCard" rel="wikipedia" title="HCard"&gt;
     hCard
    &lt;/a&gt;
    ), because I am after all the sole author of this website.
   &lt;/p&gt;
   &lt;div class="zemanta-pixie"&gt;
    &lt;img alt="" class="zemanta-pixie-img" src="http://img.zemanta.com/pixy.gif?x-id=9e03fdb6-c5dc-4878-b734-7bec34cd9ced"/&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Gamification of book reading</title><link href="/gamification-of-book-reading.html" rel="alternate"></link><updated>2011-05-11T17:15:00+02:00</updated><author><name>markos</name></author><id>tag:,2011-05-11:gamification-of-book-reading.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I’ve been following subjects of
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Gamification" rel="wikipedia" title="Gamification"&gt;
     gamification
    &lt;/a&gt;
    and
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Game_mechanics" rel="wikipedia" title="Game mechanics"&gt;
     game mechanics
    &lt;/a&gt;
    for a couple of years now, but I’ve only recently spotted how they apply to me. I conceitedly thought that they have little influence on me, but reality is as always more nuanced.
   &lt;/p&gt;
   &lt;p&gt;
    I still seem to be unmoved by most attempts. I don’t care that my
    &lt;a class="zem_slink" href="http://www.linkedin.com" rel="homepage" title="LinkedIn"&gt;
     LinkedIn
    &lt;/a&gt;
    profile is not complete and no fake mayorship will get me to use
    &lt;a class="zem_slink" href="http://www.foursquare.com/" rel="homepage" title="Foursquare"&gt;
     Foursquare
    &lt;/a&gt;
    until I see some other tangible benefit. On the other hand keeping a list of books and having a goal on how many to read has indeed influenced my behavior.
   &lt;/p&gt;
   &lt;p&gt;
    So why did this last example work? Because I actually care about reading books. It’s something I enjoy and want to do more, so “achievements” actually mean something to me.
   &lt;/p&gt;
   &lt;p&gt;
    I started to track books I read because I wanted to know how many I actually read, in what order and does that say something about me. I also thought setting an annual goal would provide a good incentive to keep reading, which it did, but not exactly in a way I expected.
   &lt;/p&gt;
   &lt;p&gt;
    Having a goal certainly helped me with additional motivation to read books when otherwise I might not have for one reason or another, but it also  gave me a nudge against dropping books I didn’t like since doing so would mean time invested in them would not get counted to achieving my goal.
   &lt;/p&gt;
   &lt;p&gt;
    It also pushed me to thinner books. Less pages mean on average quicker completion. But there are books where six or eight hundred pages doesn’t feel too long and they shouldn’t be postponed just because it might take me more than a couple of weeks to read them.
   &lt;/p&gt;
   &lt;p&gt;
    Time pressure also leads to more cursory reading. I’ve never been able to read as deeply as I thought I should, but having a goal definitely changed my reading for the worse. Again, properly engaging a book takes more time and hence makes it harder to achieve the goal.
   &lt;/p&gt;
   &lt;p&gt;
    I didn’t notice any downsides from having a list itself which has indeed provided me with insights about me I probably could have if I wrote a diary. Which I don’t, so I’m left to piecing my long term memories mostly from circumstantial evidence.
   &lt;/p&gt;
   &lt;p&gt;
    That’s why I’m keeping a list, but ditching my goal. Upside is simply not worth the downsides.
   &lt;/p&gt;
   &lt;p&gt;
    I see my experience as providing a lesson. It’s relatively easy to go through different gaming mechanisms and find a combination that you think will give you results you want. However, incentives are a bitch and I think it’s practically impossible to reliably predict what will actually happen in use.
   &lt;/p&gt;
   &lt;p&gt;
    That’s why complex modern games take so much time to create, because getting things right takes time and is not something you can reason through.
   &lt;/p&gt;
   &lt;p&gt;
    I’m with
    &lt;a class="zem_slink" href="http://headrush.typepad.com" rel="homepage" title="Kathy Sierra"&gt;
     Kathy Sierra
    &lt;/a&gt;
    on this and you really ought to read
    &lt;a href="http://radar.oreilly.com/2011/04/gamification-purpose-marketing.html#comment-7250008" title="Kathy Sierra's comments"&gt;
     her comments
    &lt;/a&gt;
    . I’m skeptical of general awe around gamification, but I do think game mechanics will have an important role to play in our future. Still I would prefer if we cautiously applied new tools where they can’t hurt much before we plaster badges of suck everywhere.
   &lt;/p&gt;
   &lt;p&gt;
    For a while now I’ve been annoyed by how every gamification talk has about 20 minutes of gushing of its virtues and about 20 seconds on possible ethics problems, but my experience has reminded me of the deeper problem, which is that it is simply much easier to inflict damage to your users and your product (brand) than getting it right.
   &lt;/p&gt;
   &lt;div class="zemanta-pixie"&gt;
    &lt;a class="zemanta-pixie-a" href="http://www.zemanta.com/" title="Enhanced by Zemanta"&gt;
     &lt;img alt="Enhanced by Zemanta" class="zemanta-pixie-img" src="http://img.zemanta.com/zemified_e.png?x-id=20efb278-1d20-46ec-aeec-c311b50837c7"/&gt;
    &lt;/a&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Facebook, web, mobile and me</title><link href="/facebook-web-mobile-and-me.html" rel="alternate"></link><updated>2009-12-20T16:35:00+01:00</updated><author><name>markos</name></author><id>tag:,2009-12-20:facebook-web-mobile-and-me.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Recently, for a moment, I couldn’t imagine myself working on web in ten years time. Since I am quite passionate about it and its development, the obvious question was why?
   &lt;/p&gt;
   &lt;p&gt;
    Why wouldn’t I want to be a web developer anymore?
   &lt;/p&gt;
   &lt;p&gt;
    Popularity of Facebook comes to mind. It is no secret that I am not a fan of
    &lt;a class="zem_slink" href="http://facebook.com" rel="homepage" title="Facebook"&gt;
     Facebook
    &lt;/a&gt;
    and similar networks. Yet I do have an account there and on some of its brethren. By the way, my Facebook account is for testing purposes, so if I haven’t befriended you, it almost certainly wasn’t personal.
   &lt;/p&gt;
   &lt;p&gt;
    Anyhow, Facebook and ilk are easy to criticize. Anything of that size will certainly gain much to be critical about. I also sometimes feel nostalgic about old web, before the gold rush of last millennium. That too probably says more about my age than anything else.
   &lt;/p&gt;
   &lt;p&gt;
    After some pondering I came to conclusion that I am longing for a sense of openness, equality and control I used to have. Few of us had our own server, but in principle with investment of little time and a bit more money, you could run all services that mattered.
   &lt;/p&gt;
   &lt;p&gt;
    Few actually did, since on the whole we didn’t do much. We do more now, some of which we don’t want public. I may not like Facebook’s walled garden of information, but I would have to be mad to argue for open access to all that private information. Even with existing restrictions in place people tend to trip and reveal? information to unintended audience.
   &lt;/p&gt;
   &lt;p&gt;
    If sharing our lives is what we want to do, which clearly is the case, then rise of brokers like Facebook feels as an unavoidable consequence. However that doesn’t mean natural development of a domineering agent should also be welcomed.
   &lt;/p&gt;
   &lt;p&gt;
    If at some point Facebook becomes a conduit for most of our digital lives, what happens if they revoke our account? To whom can we appeal? Who has access to the history of all our actions and what happens to it if we do decide to leave? Is it ever really gone? How do we know that our answers to this questions are true?
   &lt;/p&gt;
   &lt;p&gt;
    It’s a clich? that technology is changing societies faster then they are able to adapt. It is a clich?, but that doesn’t make it wrong. We are changing long before we are able to understand what those changes will bring down the road. It can be argued either way, but it is your outlook on world that will largely form your opinion on desirability of end result.
   &lt;/p&gt;
   &lt;p&gt;
    I am not an optimist.
   &lt;/p&gt;
   &lt;p&gt;
    I like building things and I enjoy it even more when others use them too. I am sometimes lazy and perpetually busy, so I avoid building things which not even I asked for. I always tried to incorporate a reasonable amount of security in services I have built, but I have never added transparency and accountability to the system without outside pressure. I don’t know many who would and that pressure tends to come from legal requirements. It’s a subject too dull to care about without painful personal experience.
   &lt;/p&gt;
   &lt;p&gt;
    Another reason for uneasiness comes from the other side of web connection – web browsers. Last few years have brought an incredible amount of development and most of us aren’t well acquainted with new technologies yet.
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/HTML_5" rel="wikipedia" title="HTML 5"&gt;
     HTML5
    &lt;/a&gt;
    , CSS3 and Javascript will trounce Flash and Silverlight, right?
   &lt;/p&gt;
   &lt;p&gt;
    They might, but they all might not matter. Two raising trends I find interesting are
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Mobile_Web" rel="wikipedia" title="Mobile Web"&gt;
     mobile web
    &lt;/a&gt;
    access and mobile application platforms.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;a class="zem_slink" href="http://www.apple.com/iphone" rel="homepage" title="iPhone"&gt;
     iPhone
    &lt;/a&gt;
    and
    &lt;a class="zem_slink" href="http://code.google.com/android/" rel="homepage" title="Android"&gt;
     Android
    &lt;/a&gt;
    have more than 100 000 applications between them and my impression of these ecosystems is that many of them are not much more than platform specific clients to web services. Even if most of them might not have much to do with web (games, books…), it’s not difficult to imagine a future, where most of us would access web through mobile devices and developing native clients for a new service would have a priority over access through browser.
   &lt;/p&gt;
   &lt;p&gt;
    I can’t say this would necessarily be a bad thing, since web development can be quite messy and frustrating. It probably would be less egalitarian and less sharing environment and I would certainly miss that.
   &lt;/p&gt;
   &lt;p&gt;
    At the end of this long self-indulgent post I confess I have no answers. I barely have questions and I had to write this far to get to them. It is for me the most introspective part of the year, so I might be thinking about a non-issue. Time will tell. Eventually.
   &lt;/p&gt;
   &lt;div class="zemanta-pixie"&gt;
    &lt;a class="zemanta-pixie-a" href="http://reblog.zemanta.com/zemified/a5117991-c107-4748-a6c7-31134e58e25b/" title="Reblog this post [with Zemanta]"&gt;
     &lt;img alt="Reblog this post [with Zemanta]" class="zemanta-pixie-img" src="http://img.zemanta.com/reblog_e.png?x-id=a5117991-c107-4748-a6c7-31134e58e25b"/&gt;
    &lt;/a&gt;
    &lt;span class="zem-script paragraph-reblog"&gt;
     &lt;script src="http://static.zemanta.com/readside/loader.js" type="text/javascript"&gt;
     &lt;/script&gt;
    &lt;/span&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>IE6 - hope of web standards</title><link href="/ie6-hope-of-web-standards.html" rel="alternate"></link><updated>2009-01-20T20:49:00+01:00</updated><author><name>markos</name></author><id>tag:,2009-01-20:ie6-hope-of-web-standards.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I often find myself in a room with web developers and at least one of them proclaiming that IE6 sucks because it doesn’t support web standards while others nod in agreement.
   &lt;/p&gt;
   &lt;p&gt;
    This has bothered me enough to contemplate having a talk at
    &lt;a href="http://www.barcamp.si/"&gt;
     BarCamp Ljubljana
    &lt;/a&gt;
    . I didn’t, but this is what I would say.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Internet_Explorer_6" rel="wikipedia" title="Internet Explorer 6"&gt;
     Internet Explorer 6
    &lt;/a&gt;
    was released on 27th August 2001, when twin towers were still standing in NY.
   &lt;/p&gt;
   &lt;p&gt;
    7 years is a long time to remember, but what did we work with back then? There was no
    &lt;a class="zem_slink" href="http://www.apple.com/safari/" rel="homepage" title="Safari (web browser)"&gt;
     Safari
    &lt;/a&gt;
    and Firefox was still in its infacy, far from 1.0 release coming 3 years later. It was changing so often most developers simply ignored it. Opera was at version 5 (version 6 came out in November of that year) and I am sure I am not alone in remembering how awful it was at least until version 7 in 2003.
    &lt;a class="zem_slink" href="http://mozilla.com" rel="homepage" title="Mozilla"&gt;
     Mozilla
    &lt;/a&gt;
    was at version 0.6 and although showing great promise, IE6 definitely was the browser we hoped others would match.
   &lt;/p&gt;
   &lt;p&gt;
    Back then no browser supported CSS2 in a meaningful capacity. Opera had the best support for CSS until then, but IE6 surpassed it. It didn’t support DOM Level 2, which was standardized only 4 months before first beta of IE6. Good excuse not available anymore to
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Internet_Explorer_7" rel="wikipedia" title="Internet Explorer 7"&gt;
     IE7
    &lt;/a&gt;
    and IE8. Apart from that it supported more or less everything web developer could want back then and then some. IE6 problem isn’t so much that it didn’t support standards of its time as it is that it doesn’t support standards of today.
   &lt;/p&gt;
   &lt;p&gt;
    So why the hostility?
   &lt;/p&gt;
   &lt;p&gt;
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Lancia_Delta" rel="wikipedia" title="Lancia Delta"&gt;
     Lancia Delta Integrale
    &lt;/a&gt;
    , which dominated rally competitions in eighties, is today an obsolete car, but car aficionados still have a soft sport for it. You don’t hear them go around complaining how it doesn’t meet standards of todays cars.
   &lt;/p&gt;
   &lt;p&gt;
    Well, as I’ve said, 7 years is a long time and most web developers are not old gits like me, so the historic context isn’t there. We also have to live with its real shortcomings today. But I think there’s more to this.
   &lt;/p&gt;
   &lt;p&gt;
    Complaining about IE6 standards deficiencies is declaring who you are and where you stand. It’s a simple and very recognizable way of declaring you are a member of web-standards tribe. IE6 bitching has at least as much to do with proclaimer as with proclamation’s subject. Trashing IE6 isn’t so much talking about the browser as it is talking about you.
   &lt;/p&gt;
   &lt;p&gt;
    However it is not unreasonable to ask why IE6 doesn’t better support standards that were released or updated after its initial release.
   &lt;/p&gt;
   &lt;p&gt;
    Certainly more could be done through software updates, but are standards themselves forward looking in a way that would avoid updating browser internals with every new version of a particular standard?
   &lt;/p&gt;
   &lt;p&gt;
    I am quite certain that this isn’t always possible. I don’t see how you could add a video object from
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/HTML_5" rel="wikipedia" title="HTML 5"&gt;
     HTML5
    &lt;/a&gt;
    to a browser without explicit support from browser’s developers. But not all changes are such as
    &lt;a href="http://www.alistapart.com/articles/semanticsinhtml5"&gt;
     John Allsopp
    &lt;/a&gt;
    has recently pointed out.
   &lt;/p&gt;
   &lt;p&gt;
    I have mixed feelings about HTML5. It’s bringing much needed new capabilities and lots of goodies, but I also think
    &lt;a href="http://intertwingly.net/blog/2009/01/15/Grandiosity"&gt;
     Sam Ruby
    &lt;/a&gt;
    has a point. New, simple,
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Document_Type_Declaration" rel="wikipedia" title="Document Type Declaration"&gt;
     DOCTYPE
    &lt;/a&gt;
    is reduced to a switch between standards and
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/Quirks_mode" rel="wikipedia" title="Quirks mode"&gt;
     quirks mode
    &lt;/a&gt;
    and unless it will become more complicated in later revisions, it won’t offer an easy way to distinguish between HTML5 documents and other HTML versions. This isn’t a particular problem if newer revisions will be backwards compatible with older, but this isn’t true even of HTML5. Why would we think this is the last version where we will make a major misstep that needs backwards-incompatible correction?
   &lt;/p&gt;
   &lt;p&gt;
    There are other things I don’t like about HTML5, but if I would have to pick one thing I would like to see changed, is to make it more extendable.
   &lt;/p&gt;
   &lt;p&gt;
    To sum it up, IE6 is indeed a problem in todays development, but it’s also at least at fault for this. It was an excellent browser when it came out and is remarkably solid one for software so old. There are many reasons, completely untouched in this post, why its users don’t migrate to something newer, but we, web developers, are also to blame, because we tend to standardize only current practice and leave few doors open for future development which invariably comes.
   &lt;/p&gt;
   &lt;div class="zemanta-pixie"&gt;
    &lt;a class="zemanta-pixie-a" href="http://reblog.zemanta.com/zemified/1235c7ef-00b0-4968-aaa8-90dfe88b2f35/" title="Zemified by Zemanta"&gt;
     &lt;img alt="Reblog this post [with Zemanta]" class="zemanta-pixie-img" src="http://img.zemanta.com/reblog_e.png?x-id=1235c7ef-00b0-4968-aaa8-90dfe88b2f35"/&gt;
    &lt;/a&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Switched...to Firefox 3</title><link href="/switchedto-firefox-3.html" rel="alternate"></link><updated>2008-06-22T15:04:00+02:00</updated><author><name>markos</name></author><id>tag:,2008-06-22:switchedto-firefox-3.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Today I switched to
    &lt;a class="zem_slink" href="http://www.firefox.com/" rel="homepage" title="Mozilla Firefox"&gt;
     Firefox 3
    &lt;/a&gt;
    as my main browser. I didn’t want to, because as a policy I try to use most common version as default, but I made a mistake of opening version 3 and since then none of my old add-ons seem to work on previous version anymore.
   &lt;/p&gt;
   &lt;p&gt;
    If I was still a young pup, I would probably dive into config files, search
    &lt;a class="zem_slink" href="http://en.wikipedia.org/wiki/World_Wide_Web" rel="wikipedia" title="World Wide Web"&gt;
     the web
    &lt;/a&gt;
    for answers and try to fix it. But when you get to 34, time becomes too precious to muck about needlessly and you just want to focus on things you need or better yet want to do.
   &lt;/p&gt;
   &lt;p&gt;
    So version three it is. This has got me thinking about something else. It’s remarkable how difficult or simply an annoyance it still is to have two version of a browser installed. Clearly this should be possible and is in interest of everyone involved.
   &lt;/p&gt;
   &lt;p&gt;
    Anyway, enough of ranting. Time to get back to cutting cake…
   &lt;/p&gt;
   &lt;div class="zemanta-pixie" style="margin-top: 10px; height: 15px;"&gt;
    &lt;a class="zemanta-pixie-a" href="http://reblog.zemanta.com/zemified/79c05cbe-847f-4fa0-8dc3-878f8e79208f/" title="Zemified by Zemanta"&gt;
     &lt;img alt="Zemanta Pixie" class="zemanta-pixie-img" src="http://img.zemanta.com/reblog_a.png?x-id=79c05cbe-847f-4fa0-8dc3-878f8e79208f" style="border: medium none; float: right;"/&gt;
    &lt;/a&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Public social graph</title><link href="/public-social-graph.html" rel="alternate"></link><updated>2007-08-19T00:00:00+02:00</updated><author><name>markos</name></author><id>tag:,2007-08-19:public-social-graph.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I’ve been following sporadically discussions about opening and sharing social networks’ social graphs. It’s hard to be a user of web services these days and not wish it was easier to recreate network of friends on new ones. Following sporadically means my opinion on subject may be more firm than right. I guess that makes it a perfect blogging material.
   &lt;/p&gt;
   &lt;p&gt;
    Latest article I’ve read on the subject
    &lt;a href="http://bradfitz.com/social-graph-problem/"&gt;
     comes
    &lt;/a&gt;
    from Brad Fitzpatrick, someone always worth listening to. It’s an interesting post and Brad has obviously thought about privacy issues, but there’s one conceptual problem I still don’t see resolved.
   &lt;/p&gt;
   &lt;p&gt;
    It sounds paradoxical, but I don’t think public and private data have an empty intersection. If for some reason you want to have a pseudonymous account on social network cooperating in this scheme, how can you reliably avoid being discovered? If you, for example, use same email address as with other services, then anyone using social graph data can find you out by matching hash values.
   &lt;/p&gt;
   &lt;p&gt;
    The only solution I can think of is to use a different email address, but this is neither particularly scalable and you have to decide upfront about how you intent to use the service. It also won’t work for sites, where you are already registered. There you can only hope you’ll be asked for consent before that data is given out.
   &lt;/p&gt;
   &lt;p&gt;
    Still, I’m more or less convinced that sharing will happen. Current situation is simply to painful for everyone involved (apart from biggest players) to persevere. I just don’t know what the downsides will be to which I’ll have to adapt.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;em&gt;
     Update: More on the subject from
    &lt;/em&gt;
    &lt;em&gt;
     &lt;a href="http://www.dehora.net/journal/2007/08/buddycrap.html"&gt;
      Bill de hÓra
     &lt;/a&gt;
    &lt;/em&gt;
    &lt;em&gt;
     . He says exactly what I hope I would be thinking, if I was actually doing the thinking.
    &lt;/em&gt;
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Information retrieval</title><link href="/information-retrieval.html" rel="alternate"></link><updated>2007-08-15T11:21:00+02:00</updated><author><name>markos</name></author><id>tag:,2007-08-15:information-retrieval.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I’ve just read
    &lt;a href="http://www.amazon.com/Information-Retrieval-Algorithms-Heuristics-2nd/dp/1402030045"&gt;
     Information Retrieval: Algorithms and Heuristics
    &lt;/a&gt;
    and it’s a great introduction to technics and problems encountered in the field. It didn’t make me an expert, but I can fake one at parties. If you have time and interest, I certainly recommend getting a copy.
   &lt;/p&gt;
   &lt;p&gt;
    I missed one thing. I’ve been thinking and reading about search engines for a while now, as time permits, and the underlying premise of all (known to me) seems to be to find a document that answers your query.  What happens if such document doesn’t exist, but the answer can be constructed from multiple sources?
   &lt;/p&gt;
   &lt;p&gt;
    I know this problem has been explored, but being a layman I would welcome at least few pages that described what has been done so far and where to look for more information. Any recommendation for a book or a website that addresses this topic is most welcome.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>WWW2007</title><link href="/www2007.html" rel="alternate"></link><updated>2007-05-11T18:17:00+02:00</updated><author><name>markos</name></author><id>tag:,2007-05-11:www2007.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    &lt;a href="http://www2007.org/"&gt;
     WWW2007
    &lt;/a&gt;
    is wrapping up tomorrow, but today is the last day I am attending. It’s been really refreshing to be at a web oriented gathering without hearing anybody say: “Web 2.0″.
   &lt;/p&gt;
   &lt;p&gt;
    I enjoyed listening to talks and learned that I have to brush up my math and maybe learn a new thing or two. I still learned a lot despite my personal shortcomings.
   &lt;/p&gt;
   &lt;p&gt;
    My original reason for attending was
    &lt;a href="http://airweb.cse.lehigh.edu/2007/"&gt;
     AIRWeb07
    &lt;/a&gt;
    , a workshop on adversarial information retrieval on web, because I wanted to learn what anti-spam community is working on and what sort of ideas and tools are being discussed.
   &lt;/p&gt;
   &lt;p&gt;
    The answer to an outsider like me seems mainly to be a problem of web spam polluting search engines and algorithms trying to discover spam pages from graphs of connected pages.
   &lt;/p&gt;
   &lt;p&gt;
    It is a very simplified description, probably even too simplified, and I have to say that I admire the work and ideas done and shown at the conference. At the same time I was very surprised that focus has been so much on links and practically none on characteristics of spam pages themselves (with notable exception of a submission by
    &lt;a href="http://plg.uwaterloo.ca/~gvcormac/"&gt;
     Gordon Cormack
    &lt;/a&gt;
    ). I understand that crawlers are somewhat out of vogue in research community, but it still seemed kind of odd.
   &lt;/p&gt;
   &lt;p&gt;
    It is as if someone trying to know me would completely ignore what I say and solely concentrate on who am I hanging out with and how do I go about it. It seemed to me that the best way would be to combine both, which is what prof. Cormack proposed as well.
   &lt;/p&gt;
   &lt;p&gt;
    In a way I think I may understand this focus. It’s very easy to add spam links on an open participatory web and obviously nobody can control what gets published on all those pages out there. So it’s natural for search companies to concentrate on points that they do control, like their index.
   &lt;/p&gt;
   &lt;p&gt;
    Still, I feel as if not enough has been done on preventing stuff like comment spam. Better tools would only lead to more isolated spam pages and therefore to an easier extraction of spam networks from an index.
   &lt;/p&gt;
   &lt;p&gt;
    Then again, I don’t do this kind of research for a living and may be only talking nonsense.
   &lt;/p&gt;
   &lt;p&gt;
    The other thing that’s been on my mind is a general agreement that this is an arms race and one which won’t be won unless financial incentives driving spammers can be eliminated. Question came up if we should try to break our own protections (as cryptographers do) and how would we fare?
   &lt;/p&gt;
   &lt;p&gt;
    I have no doubt that people I met fighting spam are much smarter than most of their opponents, which is why I don’t doubt that they could do it. I doubt only if they should do it.
   &lt;/p&gt;
   &lt;p&gt;
    It’s not really intuitive that the best thing in an arms race would be to show the other side how to build a nuke. Especially when it seems we are busy enough fighting problem in its current variations. Then again, I’m not convinced it’s a bad idea either.
   &lt;/p&gt;
   &lt;p&gt;
    I won’t go much into other talks, as I feel even less qualified to judge and anyone can get a better insight just by reading accepted papers that are all available online. What I heard or read has been in general of a high quality and if this year is anything to go by, then try to attend next one, which will be next April in Beijing.
   &lt;/p&gt;
   &lt;p&gt;
    By the way, visiting Banff is a must even without a conference as an excuse, since the place and even more its surroundings are just stunning. And if you do decide to go, do yourself a favor and contact
    &lt;a href="http://www.tarry.ca/"&gt;
     Alicyn
    &lt;/a&gt;
    for a place to stay.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Transient nature of OpenID identities</title><link href="/transient-nature-of-openid-identities.html" rel="alternate"></link><updated>2007-04-15T20:51:00+02:00</updated><author><name>markos</name></author><id>tag:,2007-04-15:transient-nature-of-openid-identities.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I’m still a fan of OpenID, even though it lacks solid solutions to some of its problems.
   &lt;/p&gt;
   &lt;p&gt;
    Much has been said about risk of phishing and possible solutions, but I’ve been more ignorant than some to a transient nature of OpenID identities. Simply put, even if you run your own server, you don’t own your domain. You just rent it. Thus you don’t really own your OpenID URL either, which makes it a rather risky tool where some sort of permanent identity is needed.
   &lt;/p&gt;
   &lt;p&gt;
    There are plenty of cases where there’s no such need and OpenID is in my opinion a better solution to them than existing ones. But it isn’t good enough where something less fleeting is required. Maybe
    &lt;a href="http://www.xdi.org/"&gt;
     XRI/XDI
    &lt;/a&gt;
    is a better answer?
   &lt;/p&gt;
   &lt;p&gt;
    I don’t know. I’m still a fan of OpenID. It’s just a bit less useful than I first thought.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Handling 404</title><link href="/handling-404.html" rel="alternate"></link><updated>2006-11-08T12:31:00+01:00</updated><author><name>markos</name></author><id>tag:,2006-11-08:handling-404.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    This blog doesn’t use descriptive URLs, which is not the only time I screwed this up for no good reason. In this case it was mainly laziness and smug i-don’t-care-if-people-find-me attitude, but I hadn’t really realized how stupid this decision was until I started thinking about the problem of missing pages (error 404).
   &lt;/p&gt;
   &lt;p&gt;
    It always bugged me how useless most 404 pages are. Sure, I could notify the webmaster about a broken link, but that won’t help me find what I’m looking for. Can’t we do better than that?
   &lt;/p&gt;
   &lt;p&gt;
    As it happens we often can.  There’s an unlimited number of ways in which visitors can fail to reach their destination, but majority of them probably fall in four categories:
   &lt;/p&gt;
   &lt;ul&gt;
    &lt;li&gt;
     content moved elsewhere
    &lt;/li&gt;
    &lt;li&gt;
     broken links
    &lt;/li&gt;
    &lt;li&gt;
     typos
    &lt;/li&gt;
    &lt;li&gt;
     bad memory recall
    &lt;/li&gt;
   &lt;/ul&gt;
   &lt;p&gt;
    &lt;strong&gt;
     Content moved elsewhere
    &lt;/strong&gt;
   &lt;/p&gt;
   &lt;p&gt;
    Many believe that pages should be permanent and links shouldn’t break over time. Yet sometimes we either have little control over this or there are good reasons for breakage. We should still try to mitigate such situation by guiding visitors to correct new location when possible.
   &lt;/p&gt;
   &lt;p&gt;
    This is done with HTTP redirects. If content was moved only temporary, then response should have a status code of 302 and contain a link in the header to correct current address. If move was permanent, then the same thing is achieved with code 301.
   &lt;/p&gt;
   &lt;p&gt;
    Missing pages amount to around 0.6% of hits on this website. They would be around 8% if redirects weren’t used.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;strong&gt;
     Broken links
    &lt;/strong&gt;
   &lt;/p&gt;
   &lt;p&gt;
    Broken links appear when email clients break URL longer than maximum line length or from botched copy operation. This usually means that address is cut off and part we have is incomplete, but otherwise correct. Handling such links can range from trivial to impossible. Let’s look at one Wikipedia link as an example:
   &lt;/p&gt;
   &lt;p style="text-indent:20pt;"&gt;
    &lt;a href="http://en.wikipedia.org/wiki/Longest_common_subsequence_problem" title="Link to article about longest common sequence problem"&gt;
     http://en.wikipedia.org/wiki/Longest_common_subsequence_problem
    &lt;/a&gt;
   &lt;/p&gt;
   &lt;p&gt;
    Let’s say that link was broken near the end of it and there was
    &lt;em&gt;
     lem
    &lt;/em&gt;
    missing in
    &lt;em&gt;
     problem
    &lt;/em&gt;
    . Resulting address might not be complete, but there probably aren’t that many Wikipedia articles where such string forms first part of their address and it would be reasonable to assume that Wikipedia could find the right article. Alas it doesn’t.
   &lt;/p&gt;
   &lt;p&gt;
    On the other side of the spectrum are impossible or almost impossible guesses. If address was broken anywhere before
    &lt;em&gt;
     Longest
    &lt;/em&gt;
    , then we could learn nothing from it about visitors expectations. This would look like a good place to give up.
   &lt;/p&gt;
   &lt;p&gt;
    However, if we are lucky, then our visitors came from one of popular search engines, which means their referrer attribute includes search string that led to our page. We can extract those keywords, use them and hopefully find that page or failing that offer a list of related pages. Not perfect, but beats plain “Not here” sign.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;strong&gt;
     Typos
    &lt;/strong&gt;
   &lt;/p&gt;
   &lt;p&gt;
    Typos are what happens when people use keyboards. I can’t live without one, but I still recognize that my fingers and my brain are not always of the same mind about how to spell a word. Letters get added, dropped or just switch places, all being a problem for a program that usually looks for that one perfect match.
   &lt;/p&gt;
   &lt;p&gt;
    There’s help. Edit distance algorithms, like
    &lt;a href="http://en.wikipedia.org/wiki/Levenshtein_distance" title="Description of Levenshteins algorithm"&gt;
     Levenshtein’s
    &lt;/a&gt;
    , can be used to measure how similar two strings really are. Matching then becomes finding the page with shortest distance from a list of its closest neighbors.
   &lt;/p&gt;
   &lt;p&gt;
    Downside is that algorithm is fairly computationally expensive and it might take time to find a match. I’ll explore this problem in tomorrows article.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;strong&gt;
     Bad memory requests
    &lt;/strong&gt;
   &lt;/p&gt;
   &lt;p&gt;
    The main purpose of descriptive URLs is the same as catchy domain names. Make it easy to remember address for later use, when bookmarks or browsers autocomplete can’t be used.
   &lt;/p&gt;
   &lt;p&gt;
    But memory is notoriously unreliable and it doesn’t work any better with web addresses. So addresses used may vary enough from the right one that they don’t get caught with edit distance algorithms. As an example, someone might try to access aforementioned Wikipedia’s article with:
   &lt;/p&gt;
   &lt;p style="text-indent:20pt;"&gt;
    http://en.wikipedia.org/wiki/Longest_subsequence_problem
   &lt;/p&gt;
   &lt;p&gt;
    Calculated distance between this address and the real one is 7, which is probably more than would be real limit for matching. We can still look at referrer for clues, but we can also use requested address. In our case, we can extract keywords
    &lt;em&gt;
     longest
    &lt;/em&gt;
    ,
    &lt;em&gt;
     subsequence
    &lt;/em&gt;
    and
    &lt;em&gt;
     problem
    &lt;/em&gt;
    and use them to search for possible hits. Wikipedia doesn’t do this either, but neither do I, so I shouldn’t complain.
    &lt;br/&gt;
    &lt;strong&gt;
     Time to wrap it up
    &lt;/strong&gt;
   &lt;/p&gt;
   &lt;p&gt;
    I believe this approaches make a good case for logical and descriptive addresses, since most of them don’t work (well) otherwise. If someone requests an article with a nonexistent ID 145, it’s impossible to resolve which of existing ones with IDs 155, 245 or 149 he really wanted.
   &lt;/p&gt;
   &lt;p&gt;
    Still, sometimes descriptive addresses are not an option. I’d love to hear ideas or practical experience of how to handle such cases.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>rtvslo.si redesign</title><link href="/rtvslosi-redesign.html" rel="alternate"></link><updated>2006-06-04T14:18:00+02:00</updated><author><name>markos</name></author><id>tag:,2006-06-04:rtvslosi-redesign.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Slovenian national television has a
    &lt;a href="http://www.rtvslo.si"&gt;
     new design
    &lt;/a&gt;
    . It’s certainly better than what they had before, but it’s far from perfect.
   &lt;/p&gt;
   &lt;p&gt;
    Fry already
    &lt;a href="http://friedcellcollective.net/outbreak/2006/06/03/validity/"&gt;
     described
    &lt;/a&gt;
    some of the graver errors. But his list is not nearly complete and here are some issues that bother me most:
   &lt;/p&gt;
   &lt;ul&gt;
    &lt;li&gt;
     not only is Javascript often inlined, it’s also not degradable (crap like href=”javascript:void really has no place in modern web development)
    &lt;/li&gt;
    &lt;li&gt;
     &lt;a href="http://en.wikipedia.org/wiki/Divitis" title="Description of divitis"&gt;
      divitis
     &lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
     inline CSS
    &lt;/li&gt;
    &lt;li&gt;
     ridiculously long list of external resources (87 on main page), which result in excruciatingly long page loads on connections with high latency
    &lt;/li&gt;
    &lt;li&gt;
     poor contrast between text and background in large parts of a page can be an accessibility problem
    &lt;/li&gt;
   &lt;/ul&gt;
   &lt;p&gt;
    I could go on, but I don’t want to complain too much. As I’ve said before, this page is a clear improvement over the previous one and it’s also obvious from it that their creators do care about modern development and its often forgotten parts like accessibility.  This is exactly the reason why I decided to write about its failings and not about some other, more atrocious one.
    &lt;a href="http://www.em3r10.com/"&gt;
     Zavod Embrio
    &lt;/a&gt;
    shows an ambition for good designs and although I don’t think they’re quite there yet, I’m sure they soon will be.
   &lt;/p&gt;
   &lt;p&gt;
    And I guess my notion that there’s no need for workshops on XHTML, CSS and web standards in general has been proven wrong.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Reusability of css based designs - a myth?</title><link href="/reusability-of-css-based-designs-a-myth.html" rel="alternate"></link><updated>2006-03-14T10:18:00+01:00</updated><author><name>markos</name></author><id>tag:,2006-03-14:reusability-of-css-based-designs-a-myth.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Have you ever created a new design for an existing website without changing XHTML templates?
   &lt;/p&gt;
   &lt;p&gt;
    I’m not talking about designs for
    &lt;a href="http://www.csszengarden.com/"&gt;
     css Zen Garden
    &lt;/a&gt;
    and its imitators or any other occasion where doing so was done to prove a point. What I mean is building a new design on existing XHTML template during normal course of action.
   &lt;/p&gt;
   &lt;p&gt;
    I haven’t and I don’t know anyone who did. It’s also hard to imagine being otherwise if designs are done without taking existing XHTML into account. There’s always an element missing for attaching a part of your design to, unless XHTML was somewhat loose to begin with.
   &lt;/p&gt;
   &lt;p&gt;
    I don’t think this is a problem. There are plenty of real benefits to table-less designs and few, if any deficiencies. However, I do believe that this benefit, touted by many including me, is contrived.
   &lt;/p&gt;
   &lt;p&gt;
    Or am I just wrong? What’s your experience?
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Firefox and defer="defer"</title><link href="/firefox-and-deferdefer.html" rel="alternate"></link><updated>2006-03-04T17:19:00+01:00</updated><author><name>markos</name></author><id>tag:,2006-03-04:firefox-and-deferdefer.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    If you’re sitting in the wrong part of the world, then Google might be inaccessible to you right now. That would be only a minor annoyance, if the same wasn’t true also for Google Analytics.
   &lt;/p&gt;
   &lt;p&gt;
    It’s remarkable how many sites (including this one) use it and are right now more or less inaccessible to every poor soul suffering from Google blackout. That’s because browser can’t tell if external Javascript will use a method like
    &lt;em&gt;
     document.write
    &lt;/em&gt;
    and therefore blocks rendering until the code is loaded or it timeouts waiting for it.
   &lt;/p&gt;
   &lt;p&gt;
    There should be a workaround for times like this and according to HTML4 specification there is
    &lt;a href="http://www.w3.org/TR/1999/REC-html401-19991224/interact/scripts.html#adef-defer"&gt;
     one
    &lt;/a&gt;
    . By using
    &lt;em&gt;
     defer=”defer”
    &lt;/em&gt;
    we can signal to the browser that it can safely proceed rendering the page, because we won’t be using any dangerous methods in our Javascript files.
   &lt;/p&gt;
   &lt;p&gt;
    However, it sadly doesn’t work everywhere. It works in Internet Explorer, but fails in Firefox and Safari. And that simply sucks.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>IE7 will remain to be MIME challenged</title><link href="/ie7-will-remain-to-be-mime-challenged.html" rel="alternate"></link><updated>2005-12-07T11:49:00+01:00</updated><author><name>markos</name></author><id>tag:,2005-12-07:ie7-will-remain-to-be-mime-challenged.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    IE7
    &lt;a href="http://blogs.msdn.com/ie/archive/2005/09/15/467901.aspx" title="Link to IEBlog"&gt;
     won’t
    &lt;/a&gt;
    support proper MIME type for XHTML. Too bad, since it will permit people like
    &lt;a href="http://www.iprom.si/"&gt;
     these guys
    &lt;/a&gt;
    to crap on web for a while longer.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>I like SSE</title><link href="/i-like-sse.html" rel="alternate"></link><updated>2005-11-24T22:33:00+01:00</updated><author><name>markos</name></author><id>tag:,2005-11-24:i-like-sse.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    I’ve been a netizen since ’93 and I saw web for the first time in ’94, when a friend showed me
    &lt;a href="http://www.yahoo.com"&gt;
     Yahoo
    &lt;/a&gt;
    . I felt dizzy just thinking about possibilities.
   &lt;/p&gt;
   &lt;p&gt;
    I had no idea what was coming, but even though web came a long way since then, I never felt so excited again up until now. It’s hard to predict future and I avoid revealing my stupidity if possible, but it really looks like a no-brainer predicting a vastly different, more open and collaborative web in few years time than we have now.
   &lt;/p&gt;
   &lt;p&gt;
    Feeds were a big step forward. It wasn’t necessary to scrap pages to reuse information anymore. Even more importantly, feeds changed the attitude so sharing and reusing content is now again a normal and good thing.
   &lt;/p&gt;
   &lt;p&gt;
    It seems
    &lt;a href="http://www.ietf.org/html.charters/atompub-charter.html"&gt;
     atom publishing protocol
    &lt;/a&gt;
    will soon be ratified and with it a standard way of editing web resources. In fact making sharing a two-way street. The final piece was provided by
    &lt;a href="http://spaces.msn.com/members/rayozzie/PersonalSpace.aspx?_c01_blogpart=blogmgmt&amp;amp;_c=blogpart" title="Ozzie's blog"&gt;
     Ray Ozzie
    &lt;/a&gt;
    and guys at Microsoft. A synchronization protocol for RSS feeds called
    &lt;a href="http://msdn.microsoft.com/xml/rss/sse/" title="Protocol specification"&gt;
     SSE
    &lt;/a&gt;
    published under creative commons license that should be really useful for synchronizing calendars, contacts etc.
   &lt;/p&gt;
   &lt;p&gt;
    I’ve read it a couple of days ago and I couldn’t find an obvious defect in it (apart from a small error in the example at point 2.2.2, which I’m sure has been spotted since). Truth be told, I didn’t expect it to.
   &lt;/p&gt;
   &lt;p&gt;
    It’s a simple, easy to understand specification. I wish people would stop using date-time values conforming to RFC 822, but nevertheless SSE shouldn’t be difficult to implement.
   &lt;/p&gt;
   &lt;p&gt;
    It probably really is the easiest thing that could work and it’s up to programmers to show if it might be too easy. There’s always a tension between things you specify and things you leave to implementation. Freedom given can and usually is a point of misunderstandings and until there are enough implementations in the wild, it’s hard to judge the quality of compromise.
   &lt;/p&gt;
   &lt;p&gt;
    I won’t be surprised though if final specification version won’t be significantly different than current one. I’ll try to make our implementation as soon as possible and hopefully by that time there will be others to test it with.
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry><entry><title>Paul Graham agrees</title><link href="/paul-graham-agrees.html" rel="alternate"></link><updated>2005-11-19T19:19:00+01:00</updated><author><name>markos</name></author><id>tag:,2005-11-19:paul-graham-agrees.html</id><summary type="html">&lt;p&gt;&lt;html&gt;
 &lt;body&gt;
  &lt;div&gt;
   &lt;p&gt;
    Paul Graham has written another
    &lt;a href="http://www.paulgraham.com/web20.html" title="Link to essay"&gt;
     essay
    &lt;/a&gt;
    on what Web 2.0 actually means. I hate being a “me too”, but we seem to share disgust of terms Web 2.0 and AJAX and agree that Wikipedia is faulty but good enough. Worth a read, but my favorite stays
    &lt;a href="http://www.paulgraham.com/say.html" title="What you can't say"&gt;
     the same
    &lt;/a&gt;
    .
   &lt;/p&gt;
  &lt;/div&gt;
 &lt;/body&gt;
&lt;/html&gt;&lt;/p&gt;</summary></entry></feed>